{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":235513912,"sourceType":"kernelVersion"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/marcocondemipolimeni/sustainable-procurement-agent?scriptVersionId=236386079\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"**Sustainable Procurement Agent: Project Overview**\n\nThis project aims to build an intelligent agent capable of assisting with Sustainable Procurement (SP) analysis. The agent leverages multiple techniques:\n\nKnowledge Base: It utilizes internal knowledge stored in vector databases (using ChromaDB) for:\nSpecific SP criteria linked to UNSPSC codes.\nSummarized sustainability risks associated with UNSPSC Segments.\nCountry-specific sustainability risk indicators.\n\nSemantic Search: It uses embedding models (Gemini text-embedding-004) to understand natural language descriptions of procurement items and link them to the correct UNSPSC codes via semantic similarity search against a pre-computed database of official descriptions.\n\nRetrieval-Augmented Generation (RAG): It queries the vector databases to retrieve relevant criteria and risks based on the identified UNSPSC codes and project context.\n\nContextual Research: It uses the Gemini API's integrated Google Search grounding feature to find up-to-date external information (e.g., regulations, partner policies) that might not be present in the internal databases.\n\nEvaluation: It employs a pairwise evaluation mechanism (using an LLM) to compare purely internal (RAG) suggestions against suggestions augmented with external search results, allowing for reasoned prioritization.\n\nSynthesis: It uses a powerful LLM (Gemini gemini-1.5-flash-latest) to synthesize all gathered information (context, identified codes, risks, criteria, search results, evaluation) into a final, actionable recommendation.\n\nAgentic Framework (LangGraph): It orchestrates these capabilities using LangGraph, enabling multi-turn conversations, planning, autonomous tool use (following ReAct principles where applicable), and state management.\n\nThe goal is to provide users with comprehensive, traceable, and actionable SP advice tailored to their specific procurement needs.\n","metadata":{}},{"cell_type":"markdown","source":"Step 1: Project Setup and Dependencies\nThe first step involves setting up the necessary environment by installing the required Python libraries. We use uv, a fast package installer, to manage dependencies. Key libraries include:\ngoogle-generativeai: For interacting with Gemini models (embeddings, generation, search grounding).\nchromadb: For creating and managing local vector databases.\nlangchain-google-genai, langgraph, langchain-core: For building the agent framework and integrating tools.\npandas, numpy: For data manipulation.\nrequests, gdown: For downloading data files.\nscikit-learn: For calculating embedding similarity.\nWe also handle potential dependency conflicts by pinning specific versions known to cause issues in some environments (like Kaggle/Colab). Finally, we configure the Google API key, preferably using a secure method like Kaggle Secrets or environment variables.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 1 Code Block (Reviewed Installation - Including LangChain & API Key Fallback)\n# ==============================================================================\n# If in Colab or Kaggle, RESTART the runtime first for a clean state.\n# Activate your virtual environment if using one locally.\nimport os\nimport sys\nimport subprocess\n\nprint(\"\\n--- Running Step 1: Installation & Setup ---\")\n\n# Optional but recommended: Uninstall potentially conflicting packages first\nprint(\"Uninstalling conflicting packages (if necessary)...\")\n# !pip uninstall -qqy jupyterlab kfp google-cloud-bigquery ydata-profiling spacy # Example\n\n# Install 'uv', a faster pip alternative\nprint(\"\\nInstalling uv...\")\ntry:\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"uv\"], check=True)\nexcept Exception as e:\n    print(f\"Warning: uv installation failed: {e}. Falling back to pip.\")\n    # Fallback pip install for uv if needed\n    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"uv\"], check=True, capture_output=True)\n\n# Use uv to install core libraries with pinned conflicting dependencies\nprint(\"\\nInstalling required libraries (pinning langgraph versions) using uv...\")\ninstall_command = [\n    \"uv\", \"pip\", \"install\", \"--system\", \"-q\",\n    \"numpy~=1.26.4\", \"pandas==2.2.2\", \"notebook==6.5.5\",\n    \"google-generativeai~=0.5.0\", \"chromadb~=0.5.0\",\n    \"langchain-google-genai~=1.0.0\", \"langgraph~=0.1.0\", \"langchain-core~=0.2.0\",\n    \"scikit-learn\", \"python-dotenv\", \"fsspec\", \"requests\", \"pyarrow\", \"gdown\"\n]\ntry:\n    subprocess.run(install_command, check=True)\n    print(\"\\n--- Library installation command executed using uv ---\")\nexcept Exception as e:\n    print(f\"ERROR: uv pip install failed: {e}\")\n    print(\"Check installation command and environment.\")\n    # Optionally, add fallback to regular pip here if needed\n\nprint(\"Please check the output above for any errors.\")\n\n# --- Set API Key using Kaggle Secrets, please set your Secrets in ADD-ons ---\nprint(\"\\nAttempting to set GOOGLE_API_KEY from Kaggle secrets...\")\napi_key_set_from_secrets = False\ntry:\n    from kaggle_secrets import UserSecretsClient\n    try:\n        GOOGLE_API_KEY_VALUE = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n        if GOOGLE_API_KEY_VALUE: # Check if secret value is not empty\n             os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY_VALUE\n             print(\"GOOGLE_API_KEY environment variable set using Kaggle secrets.\")\n             api_key_set_from_secrets = True\n        else:\n             print(\"Warning: Kaggle secret 'GOOGLE_API_KEY' found but is empty.\")\n    except UserSecretsClient.SecretNotFoundError:\n         print(\"Warning: Kaggle secret 'GOOGLE_API_KEY' not found.\")\n    except Exception as e_secret:\n         print(f\"Warning: An error occurred retrieving the Kaggle secret: {e_secret}\")\nexcept ImportError:\n    print(\"Warning: 'kaggle_secrets' module not found. Cannot use Kaggle secrets.\")\nexcept Exception as e:\n    print(f\"Warning: An unexpected error occurred during Kaggle secret handling: {e}\")\n\n# --- Alternative: Set API Key Directly (Use ONLY if Secrets method fails/unavailable) ---\n# WARNING: Storing keys directly in code is insecure. Use Kaggle Secrets if possible.\n# Replace None with your actual key string ONLY for temporary testing if secrets fail.\nMANUAL_API_KEY = None # Set to your key string if needed: \"AIza...\"\n\nif not api_key_set_from_secrets and MANUAL_API_KEY:\n    print(\"\\nWARNING: Setting API Key directly from MANUAL_API_KEY variable. Secrets method failed or key not set.\")\n    os.environ[\"GOOGLE_API_KEY\"] = MANUAL_API_KEY\n    print(\"GOOGLE_API_KEY environment variable set using MANUAL_API_KEY.\")\nelif not api_key_set_from_secrets and not MANUAL_API_KEY:\n    # Only print error if secrets failed AND manual key is not provided\n    print(\"\\nERROR: GOOGLE_API_KEY could not be set from Secrets and MANUAL_API_KEY is not set.\")\n# --- End Alternative ---\n\n# --- Final API Key Check ---\nprint(\"\\n--- Final API Key Check ---\")\nif not os.environ.get(\"GOOGLE_API_KEY\"):\n     print(\"ERROR: GOOGLE_API_KEY is NOT set. Please ensure it's added via Kaggle Secrets or the MANUAL_API_KEY variable (for testing only).\")\nelse:\n     print(\"OK: GOOGLE_API_KEY environment variable appears to be set.\")\n\n# ==============================================================================\n# Step 1: Project Setup and Dependencies\n# ==============================================================================\n# (Assume Step 1 code was run successfully AFTER kernel restart ONCE per session)\nprint(\"\\n--- Step 1 Prerequisite Code Block (Assumed Executed) ---\")\n# ... (Full Step 1 Code as provided previously, including API Key Fallback) ...\n# ==============================================================================\n# (End of Step 1 Code Block)\n# ==============================================================================\n\n\nimport os\nimport sys # Import sys for sys.exit()\nimport subprocess\nimport google.generativeai as genai\n# ... (glm_protobuf, glm_types imports) ...\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport time # Ensure time is imported\nimport json\nimport enum\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport operator\nfrom typing import TypedDict, Annotated, Sequence, List, Dict, Any, Optional\nimport gdown\nfrom google.api_core import retry\nfrom langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage, AIMessage, SystemMessage\nfrom langchain.tools import tool\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.graph import StateGraph, END\ntry: from langgraph.prebuilt import ToolExecutor, ToolInvocation\nexcept ImportError as e: ToolExecutor, ToolInvocation = None, None\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\nimport shutil # Ensure shutil is imported\nimport traceback\n\nprint(\"\\n--- Basic Imports Completed ---\")\n# ==============================================================================\n\n# ==============================================================================\n# Configuration for Loading Existing Data (REVISED - Copy or Fail Logic)\n# ==============================================================================\nprint(\"\\n--- Configuration for Loading Existing Data (Copy or Fail Logic) ---\")\n# Set this to the path provided by Kaggle after adding the previous run's output as input\n# If set, the script will TRY to copy data from here. If copy fails, script will EXIT.\n# If None, the script will generate data locally.\nKAGGLE_INPUT_PATH = \"/kaggle/input/sustainable-procurement-agent\" # <-- Set to \"/kaggle/input/sustainable-procurement-agent\"  or None\n\n# Define LOCAL/TARGET paths (in the writable working directory)\nlocal_chroma_db_path = \"./chroma_db\"\nlocal_pkl_path = \"./unspsc_embeddings_data_full.pkl\"\ndb_filename_to_check = \"chroma_db\" # Directory name\npkl_filename_to_check = \"unspsc_embeddings_data_full.pkl\" # File name\n\n# Flag to indicate if setup should proceed\nproceed_with_execution = True\n\n# --- Logic to handle input data ---\nif KAGGLE_INPUT_PATH and isinstance(KAGGLE_INPUT_PATH, str):\n    print(f\"Kaggle input path specified: {KAGGLE_INPUT_PATH}\")\n    if not os.path.exists(KAGGLE_INPUT_PATH):\n        print(f\"ERROR: Specified KAGGLE_INPUT_PATH does not exist: {KAGGLE_INPUT_PATH}\")\n        proceed_with_execution = False\n    else:\n        source_db_path = os.path.join(KAGGLE_INPUT_PATH, db_filename_to_check)\n        source_pkl_path = os.path.join(KAGGLE_INPUT_PATH, pkl_filename_to_check)\n\n        # --- Clean local paths BEFORE copying ---\n        print(f\"Cleaning local target directory: {local_chroma_db_path}\")\n        shutil.rmtree(local_chroma_db_path, ignore_errors=True)\n        print(f\"Cleaning local target file: {local_pkl_path}\")\n        try:\n            os.remove(local_pkl_path)\n        except OSError:\n            pass # Ignore error if file doesn't exist\n\n        # --- Attempt to COPY Chroma DB from input to local ---\n        print(f\"Attempting to copy DB from '{source_db_path}' to '{local_chroma_db_path}'...\")\n        copy_db_successful = False\n        try:\n            print(\"Waiting 2 seconds before DB copy attempt...\")\n            time.sleep(2)\n            shutil.copytree(source_db_path, local_chroma_db_path)\n            print(f\"Successfully copied ChromaDB from input.\")\n            copy_db_successful = True\n        except FileNotFoundError:\n            print(f\"ERROR: Source DB directory not found at '{source_db_path}'. Cannot proceed.\")\n            proceed_with_execution = False\n        except Exception as e_copy_db:\n            print(f\"ERROR copying ChromaDB from input: {e_copy_db}\")\n            proceed_with_execution = False\n\n        # --- Attempt to COPY PKL file from input to local (only if DB copy was ok) ---\n        if proceed_with_execution:\n            print(f\"Attempting to copy PKL from '{source_pkl_path}' to '{local_pkl_path}'...\")\n            copy_pkl_successful = False\n            try:\n                # time.sleep(1) # Optional delay\n                shutil.copy2(source_pkl_path, local_pkl_path)\n                print(f\"Successfully copied PKL file from input.\")\n                copy_pkl_successful = True\n            except FileNotFoundError:\n                print(f\"ERROR: Source PKL file not found at '{source_pkl_path}'. Cannot proceed.\")\n                proceed_with_execution = False\n            except Exception as e_copy_pkl:\n                print(f\"ERROR copying PKL file from input: {e_copy_pkl}\")\n                proceed_with_execution = False\n\n        if proceed_with_execution:\n             print(\"\\nData successfully copied from input path.\")\n        else:\n             print(\"\\nERROR: Failed to copy required data from input path. Stopping execution.\")\n             # sys.exit(\"Stopping due to failed data copy from input.\") # Use this to halt execution\n\nelse:\n    # This block executes if KAGGLE_INPUT_PATH is None or invalid\n    print(\"KAGGLE_INPUT_PATH not set or invalid. Will generate data locally.\")\n    print(f\" - Target ChromaDB: {local_chroma_db_path}\")\n    print(f\" - Target PKL File: {local_pkl_path}\")\n    # Clean local paths to ensure fresh generation\n    print(\"\\n--- Cleaning local directories for fresh generation ---\")\n    shutil.rmtree(local_chroma_db_path, ignore_errors=True)\n    try: os.remove(local_pkl_path)\n    except OSError: pass\n    print(\"Local paths cleaned.\")\n\n# Set the paths to use for the rest of the script (ALWAYS LOCAL)\nchroma_db_path_to_use = local_chroma_db_path\npkl_path_to_use = local_pkl_path\nprint(f\"\\nScript will use ChromaDB path: {chroma_db_path_to_use}\")\nprint(f\"Script will use PKL path: {pkl_path_to_use}\")\n\n# --- Exit if copy from input failed ---\nif not proceed_with_execution:\n     print(\"\\nHalting execution because input data could not be copied.\")\n     # In a real script you might raise an Exception or use sys.exit()\n     # For notebook environment, we just stop proceeding with subsequent steps\n# ==============================================================================\n# ==============================================================================\n# Restore Step 4 DB from Uploaded Zip (EDIT PATH) (Added zipfile import)\n# ==============================================================================\nprint(\"\\n--- Checking for Uploaded Step 4 DB Backup Zip ---\")\nimport zipfile # Add import here\n\n# --- !!! EDIT THIS PATH to where you uploaded chroma_db_backup_step4.zip !!! ---\nUPLOADED_ZIP_FILE_PATH = \"/kaggle/input/back-up\" # Example path - CHANGE THIS\n# --- End Edit ---\n\nLOCAL_DB_TARGET_PATH_S4 = \"./chroma_db\" # Target directory for Step 4 DB\n\nrestored_from_zip = False\nif UPLOADED_ZIP_FILE_PATH and os.path.exists(UPLOADED_ZIP_FILE_PATH):\n    print(f\"Found uploaded zip file: {UPLOADED_ZIP_FILE_PATH}\")\n    print(f\"Attempting to restore Step 4 DB to: {LOCAL_DB_TARGET_PATH_S4}\")\n    try:\n        # Clean target directory before unzipping\n        print(f\" > Cleaning target directory '{LOCAL_DB_TARGET_PATH_S4}'...\")\n        shutil.rmtree(LOCAL_DB_TARGET_PATH_S4, ignore_errors=True)\n        os.makedirs(LOCAL_DB_TARGET_PATH_S4, exist_ok=True) # Recreate directory\n\n        # Unzip the backup\n        print(f\" > Unzipping '{UPLOADED_ZIP_FILE_PATH}'...\")\n        with zipfile.ZipFile(UPLOADED_ZIP_FILE_PATH, 'r') as zip_ref: # Uses zipfile\n            zip_ref.extractall(LOCAL_DB_TARGET_PATH_S4) # Extract into the target dir\n\n        # Verify extraction (basic check for sqlite file)\n        if os.path.exists(os.path.join(LOCAL_DB_TARGET_PATH_S4, \"chroma.sqlite3\")):\n            print(\" > Successfully unzipped and restored Step 4 DB backup.\")\n            restored_from_zip = True\n        else:\n            print(\" > WARNING: Unzip completed, but chroma.sqlite3 not found in target. Restoration might be incomplete.\")\n\n    except zipfile.BadZipFile: # Uses zipfile\n        print(f\"ERROR: The file '{UPLOADED_ZIP_FILE_PATH}' is not a valid zip file.\")\n    except Exception as e_unzip:\n        print(f\"ERROR during unzip/restore: {e_unzip}\")\nelse:\n    print(\"Uploaded Step 4 zip backup not found or path not set. Will proceed with normal Step 4 logic (may regenerate or use copied data).\")\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:42:07.949833Z","iopub.execute_input":"2025-04-27T09:42:07.951718Z","iopub.status.idle":"2025-04-27T09:42:23.615543Z","shell.execute_reply.started":"2025-04-27T09:42:07.951668Z","shell.execute_reply":"2025-04-27T09:42:23.614388Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Step 1: Installation & Setup ---\nUninstalling conflicting packages (if necessary)...\n\nInstalling uv...\n\nInstalling required libraries (pinning langgraph versions) using uv...\n\n--- Library installation command executed using uv ---\nPlease check the output above for any errors.\n\nAttempting to set GOOGLE_API_KEY from Kaggle secrets...\nGOOGLE_API_KEY environment variable set using Kaggle secrets.\n\n--- Final API Key Check ---\nOK: GOOGLE_API_KEY environment variable appears to be set.\n\n--- Step 1 Prerequisite Code Block (Assumed Executed) ---\n\n--- Basic Imports Completed ---\n\n--- Configuration for Loading Existing Data (Copy or Fail Logic) ---\nKaggle input path specified: /kaggle/input/sustainable-procurement-agent\nCleaning local target directory: ./chroma_db\nCleaning local target file: ./unspsc_embeddings_data_full.pkl\nAttempting to copy DB from '/kaggle/input/sustainable-procurement-agent/chroma_db' to './chroma_db'...\nWaiting 2 seconds before DB copy attempt...\nSuccessfully copied ChromaDB from input.\nAttempting to copy PKL from '/kaggle/input/sustainable-procurement-agent/unspsc_embeddings_data_full.pkl' to './unspsc_embeddings_data_full.pkl'...\nSuccessfully copied PKL file from input.\n\nData successfully copied from input path.\n\nScript will use ChromaDB path: ./chroma_db\nScript will use PKL path: ./unspsc_embeddings_data_full.pkl\n\n--- Checking for Uploaded Step 4 DB Backup Zip ---\nUploaded Step 4 zip backup not found or path not set. Will proceed with normal Step 4 logic (may regenerate or use copied data).\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Step 2: Load SP Criteria Data\nWith the environment set up, we load the core Sustainable Procurement criteria data. This data, expected in a CSV file (e.g., from Google Drive), contains mappings between UNSPSC codes and specific SP criteria, documentation requirements, KPIs, SDGs, etc. It forms the primary internal knowledge base for SP requirements. The code uses a robust method: downloading the file locally first (using requests or gdown) before loading it into a pandas DataFrame (df_criteria) to handle potential network issues or large file sizes effectively.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport requests # Library to download the file\nimport os # To handle file paths and cleanup\nimport time # To add a small delay before cleanup\n\n# --- Dependencies ---\n# This approach requires the 'requests' library.\n# Ensure it was installed in Step 1.\n\n# --- Configuration ---\n# Updated Google Drive link provided by user: https://drive.google.com/file/d/1PdvfklLTOxaKP1Wjl03mTpgil76WTgRn/view?usp=sharing\n# Extracting the new file ID\nfile_id = \"1PdvfklLTOxaKP1Wjl03mTpgil76WTgRn\" # <-- Updated File ID\ngdrive_url = f'https://drive.google.com/uc?export=download&id={file_id}'\nlocal_temp_filename = \"temp_criteria_download.csv\" # Temporary local file name\n\n# --- Download the File ---\nprint(f\"Attempting to download file from Google Drive URL: {gdrive_url}\")\ndf_criteria = None # Initialize dataframe variable\ndownload_successful = False\n\ntry:\n    # Use requests to get the file content, stream=True handles large files better\n    # Added a timeout to prevent hanging indefinitely\n    response = requests.get(gdrive_url, stream=True, timeout=300) # 5 minute timeout for download request\n    response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n\n    # Write the content to a local temporary file chunk by chunk\n    with open(local_temp_filename, 'wb') as f:\n        for chunk in response.iter_content(chunk_size=8192): # Download in 8KB chunks\n            f.write(chunk)\n    print(f\"File successfully downloaded to: {local_temp_filename}\")\n    download_successful = True\n\nexcept requests.exceptions.Timeout:\n    print(f\"\\n--- ERROR: Download timed out after 5 minutes. ---\")\n    print(\"Check network connection or file size/accessibility.\")\nexcept requests.exceptions.RequestException as e:\n    print(f\"\\n--- ERROR: Failed to download the file from Google Drive ---\")\n    print(e)\n    print(\"Check the URL, network connection, and file sharing permissions.\")\nexcept Exception as e:\n    print(f\"\\n--- ERROR: An unexpected error occurred during download ---\")\n    print(e)\n\n# --- Load the Data from Local File (only if download succeeded) ---\nif download_successful:\n    print(f\"\\nAttempting to load CSV from local file: {local_temp_filename}\")\n    try:\n        # Now read the downloaded local file using pandas\n        df_criteria = pd.read_csv(local_temp_filename)\n\n        # --- Verify Data Loading ---\n        print(\"\\n--- Successfully loaded SP Criteria data from local file! ---\")\n\n        # Print the first 5 rows\n        print(\"\\nFirst 5 rows of the DataFrame:\")\n        try:\n            print(df_criteria.head().to_markdown(index=False))\n        except ImportError:\n            print(df_criteria.head())\n\n        # Print the column names\n        print(\"\\nColumn names:\")\n        print(df_criteria.columns.tolist())\n\n        # Print DataFrame info\n        print(\"\\nDataFrame Info:\")\n        df_criteria.info()\n\n    except pd.errors.ParserError as e:\n        print(f\"\\n--- ERROR: Could not parse the downloaded file. ---\")\n        print(f\"It might not be a valid CSV.\")\n        print(f\"Error details: {e}\")\n        df_criteria = None # Ensure df is None on error\n\n    except Exception as e:\n        print(f\"\\n--- ERROR: An unexpected error occurred during CSV loading ---\")\n        print(e)\n        df_criteria = None # Ensure df is None on error\n\n# --- Cleanup ---\n# Attempt to delete the temporary file if it exists\nif os.path.exists(local_temp_filename):\n    print(f\"\\nAttempting cleanup of temporary file: {local_temp_filename}\")\n    # Add a small delay to ensure file handle is released if needed\n    time.sleep(1)\n    try:\n        os.remove(local_temp_filename)\n        print(f\"Temporary file '{local_temp_filename}' deleted.\")\n    except Exception as e:\n        print(f\"Warning: Could not delete temporary file '{local_temp_filename}'. Error: {e}\")\nelse:\n    if download_successful:\n         print(f\"\\nWarning: Temporary file '{local_temp_filename}' not found for cleanup, though download was reported successful.\")\n\n# --- Final Status ---\nif df_criteria is not None:\n    print(\"\\nVariable 'df_criteria' now holds the loaded data.\")\nelse:\n    print(\"\\nVariable 'df_criteria' is None due to loading or download error.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:42:23.618175Z","iopub.execute_input":"2025-04-27T09:42:23.618707Z","iopub.status.idle":"2025-04-27T09:42:30.873734Z","shell.execute_reply.started":"2025-04-27T09:42:23.618657Z","shell.execute_reply":"2025-04-27T09:42:30.872773Z"}},"outputs":[{"name":"stdout","text":"Attempting to download file from Google Drive URL: https://drive.google.com/uc?export=download&id=1PdvfklLTOxaKP1Wjl03mTpgil76WTgRn\nFile successfully downloaded to: temp_criteria_download.csv\n\nAttempting to load CSV from local file: temp_criteria_download.csv\n\n--- Successfully loaded SP Criteria data from local file! ---\n\nFirst 5 rows of the DataFrame:\n| .   |   UNSPSC | SP Category                                                 | a. Criteria Text                                                                                                                                                                                                                                                                                                                                                                              | b. Required Documentation                                                                                                                                                                                                                                                                                                                                                                                                                                   | Market Research                                                                                                               | Contract clause and KPIs                                                                                                                                                                                                                | SDG                                            |\n|:----|---------:|:------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------------------------------------------------------------------------------------------------------|:----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------|\n| ðŸ”·  | 95141904 | Labor Standards - Forced labour and human trafficking       | The bidder shall provide documentation that describes the measures taken to avoid, prevent, and eradicate forced labor and human trafficking along its supply chain                                                                                                                                                                                                                           | ðŸ˜ Basic â€“ Policy on forced labour and human trafficking in the supply chain                                                                                                                                                                                                                                                                                                                                                                                | Go to the DRiVE market analysis page and select 'q. 8 - Keeping original identification documents of your employees' in STEP2 | For the duration of the contract period, the successful bidder shall document and report on  the following aspects:                                                                                                                     | SDG 8: Decent work and economic growth         |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Sample of signed contracts with suppliers                                                                                                                                                                                             | SDG 12: Responsible consumption and production |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               | ðŸ™‚ Mature â€“ Presentation of HR Policies including clear prescriptions against forced labor /human trafficking and how it is monitored                                                                                                                                                                                                                                                                                                                       |                                                                                                                               | - Procedures in place for checking/auditing sub-contractors on forced labor and/or human trafficking                                                                                                                                    |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Records on number of hours effectively worked (per week) by each employees is available                                                                                                                                               |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               | â­ Advanced â€“ Training of staff and forced labour risk assessment/ 3rd-party social audit report of supply chain                                                                                                                                                                                                                                                                                                                                            |                                                                                                                               | - Records of all workers including their wages and benefits                                                                                                                                                                             |                                                |\n| ðŸ”·  | 95141904 | Health and Safety - Incidents/accidents tracking system     | The bidder shall provide documentation that demonstrates how accidents and incidents are tracked and documented within their organisation.                                                                                                                                                                                                                                                    | ðŸ˜ Basic â€“ Recent statistics on incidents/accidents /some type of record keeping (disaggregated by sex)                                                                                                                                                                                                                                                                                                                                                     | Go to the DRiVE market analysis page and select 'q. 45 - Accident investigations' in STEP2                                    | For the duration of the contract period, the successful bidder shall document and report on  the following aspects:                                                                                                                     | SDG 8: Decent work and economic growth         |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Records of incidents /accidents is available (disaggregated by sex)                                                                                                                                                                   |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               | ðŸ™‚ Mature â€“ Procedures of the organisation/ templates for incidents /accidents record on file/statistics                                                                                                                                                                                                                                                                                                                                                    |                                                                                                                               | - Total number of incidents of non-compliance with regulations and/or voluntary codes concerning the health and safety impacts of products and services within the year :                                                               |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | Incidents of non-compliance with regulations resulting in a fine or penalty                                                                                                                                                             |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               | â­ Advanced â€“ Organisation is compliant with ISO 45001 or equivalent/third party audits are conducted                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                               | Incidents of non-compliance with regulations resulting in a warning                                                                                                                                                                     |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | Number and rate of fatalities as a result of work-related injury                                                                                                                                                                        |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | Number and rate of high-consequence work-related injuries (excluding fatalities)                                                                                                                                                        |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | Incidents of non-compliance with voluntary codes                                                                                                                                                                                        |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | Process in place to investigate work-related incidents to determine corrective actions and identify areas of improvements                                                                                                               |                                                |\n| ðŸ”·  | 95141904 | Environmental sustainability - Product life cycle extension | A. The bidder shall demonstrate how the useful life of the offered product can be extended through relevant durability, repairability, and upgradability features of the product and through repair, refurbishment, remanufacturing and reuse during or after the contract period.                                                                                                            | ðŸ˜ Basic â€“ A written statement that includes the applicable parts that are replaceable. A guarantee of the continued availability of spare parts                                                                                                                                                                                                                                                                                                            | nan                                                                                                                           | For the duration of the contract period, the successful bidder shall document and report on the following aspects :                                                                                                                     | SDG 12: Responsible consumption and production |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Incident solved: number of incidents resolved within the incident resolution time during a month / total number of incidents opened during the given month or opened during a previous month and still pending. Monthly target: â‰¥90%. | SDG 13: Climate action                         |\n|     |          |                                                             | B. The bidder shall provide a written declaration that the products supplied will be warranted in conformity with the contractual terms and the related service level agreement, over the specified duration (X years) of the contract such as:                                                                                                                                               | ðŸ™‚ Mature â€“ Warranty / Evidence of established service for maintenance and repair                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                               | - Commitment to repair as first remedy: number of incidents resolved within a product repair or upgrade / number of incidents resolved within a product replacement.                                                                    |                                                |\n|     |          |                                                             | a. access to the manufacturer's warranty                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             | b. management of failures                                                                                                                                                                                                                                                                                                                                                                     | â­ Advanced â€“ Extended service agreement, Design for reparability (principles of modular design is applied), Ecolabels for products e.g. EPEAT, TCO Certified                                                                                                                                                                                                                                                                                               |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             | c. upgrading of product                                                                                                                                                                                                                                                                                                                                                                       |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             | d. repair/replacement activities                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             | C. The bidder shall provide a guarantee of extended support during the use of the product                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n| ðŸ”·  | 95141904 | Environmental sustainability - Sustainable packaging        | A. The bidder shall demonstrate that packaging material includes recycled content (e.g. recycled cardboard) and that use of single-use plastic is avoided or minimised                                                                                                                                                                                                                        | ðŸ˜ Basic â€“ Documentation which shows that the packaging has at least 30% recycled content / packaging is recyclable                                                                                                                                                                                                                                                                                                                                         | nan                                                                                                                           | For the duration of the contract period, the successful bidder shall document and report on the following aspects of sustainable packaging:                                                                                             | SDG 12: Responsible consumption and production |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - % of the packaging produced without the use of PVC plastics                                                                                                                                                                           | SDG 13: Climate action                         |\n|     |          |                                                             | B. The bidder shall present its current approach to packaging reduction and avoidance of single-use plastics (through an action plan) with targets/milestones and current achievement levels while ensuring the safe delivery of products.                                                                                                                                                    | ðŸ™‚ Mature â€“ A formal packaging policy and plan with reuse, reduction & recycling objectives/ annual report. Packaging should include labeling/instruction/guidance on how the packaging waste should be managed (e.g. sorting instructions, adding to mixed waste,) / packaging with at least 50% of recycled content                                                                                                                                       |                                                                                                                               | - Quantity of packaging using 70% recycled cardboard out of total packaging                                                                                                                                                             |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Measures in place to improve product-to-package ratio                                                                                                                                                                                 |                                                |\n|     |          |                                                             | C. The bidder shall ensure that the products offered shall be delivered in bulk packaging                                                                                                                                                                                                                                                                                                     | â­ Advanced â€“ A formal packaging policy & plan with reuse/ reduction & recycling objectives/ Annual report of achievements & demonstration that quantity of packaging using 70% recycled cardboard out of total packaging /third party certification/ecolabel. Packaging should include labeling/instruction/guidance on how the packaging waste should be managed (e.g. sorting instructions, adding to mixed waste,) / organic or biodegradable packaging |                                                                                                                               | - Targets for packaging reduction in place / current achievements levels                                                                                                                                                                |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n|     |          |                                                             | D. The bidder shall provide the following information: what percentage of the packaging is suitable for reuse; what percentage of the packaging is suitable for recycling; what system is in operation to reuse the packaging (e.g. a pooling system or deposit system); what system is in operation to properly recycle the packaging; whether multi-layers and composite packaging are used |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n| ðŸ”·  | 95141904 | Labor Standards - Hiring of local labour                    | The bidder shall demonstrate that there is minimum local workforce content for direct suppliers                                                                                                                                                                                                                                                                                               | ðŸ˜ Basic â€“  A report showing previous experience with the hiring and/or training of locals in a similar project / At least 5% of jobs are reserved for locals                                                                                                                                                                                                                                                                                               | nan                                                                                                                           | For the duration of the contract period, the successful bidder shall document and report on  the following aspects:                                                                                                                     | SDG 1: No poverty                              |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Percentage of workforce hired locally within local communities                                                                                                                                                                        | SDG 8: Decent work and economic growth         |\n|     |          |                                                             | The bidder shall provide examples of skills/knowledge transfer to the local workforce/communities in its area of expertise                                                                                                                                                                                                                                                                    | ðŸ™‚ Mature â€“ Formal policy and procedures for hiring and/or training of locals / At least 30% of jobs are reserved for locals /                                                                                                                                                                                                                                                                                                                              |                                                                                                                               | - KPIâ€™s % of change from the last year                                                                                                                                                                                                  |                                                |\n|     |          |                                                             |                                                                                                                                                                                                                                                                                                                                                                                               |                                                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                               | - Local awareness communication campaign on jobs                                                                                                                                                                                        |                                                |\n|     |          |                                                             | The bidder shall ensure that at least [x]% of positions will be reserved for locals in the short to medium term                                                                                                                                                                                                                                                                               | â­ Advanced â€“ An action plan for hiring and/or training of locals with measurable targets / Dedicated programme for hiring and/or training locals in project areas                                                                                                                                                                                                                                                                                          |                                                                                                                               |                                                                                                                                                                                                                                         |                                                |\n\nColumn names:\n['.', 'UNSPSC', 'SP Category', 'a. Criteria Text', 'b. Required Documentation', 'Market Research', 'Contract clause and KPIs', 'SDG']\n\nDataFrame Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 59260 entries, 0 to 59259\nData columns (total 8 columns):\n #   Column                     Non-Null Count  Dtype \n---  ------                     --------------  ----- \n 0   .                          59260 non-null  object\n 1   UNSPSC                     59260 non-null  int64 \n 2   SP Category                59260 non-null  object\n 3   a. Criteria Text           59260 non-null  object\n 4   b. Required Documentation  59260 non-null  object\n 5   Market Research            41196 non-null  object\n 6   Contract clause and KPIs   53672 non-null  object\n 7   SDG                        59260 non-null  object\ndtypes: int64(1), object(7)\nmemory usage: 3.6+ MB\n\nAttempting cleanup of temporary file: temp_criteria_download.csv\nTemporary file 'temp_criteria_download.csv' deleted.\n\nVariable 'df_criteria' now holds the loaded data.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"Step 3: Initialize Clients\nHere, we initialize the clients needed for external services and local databases:\nGoogle Generative AI Client (genai): Configured using the API key set in Step 1, this client allows interaction with Gemini models.\nEmbedding Function (gemini_ef): A specific object (GoogleGenerativeAiEmbeddingFunction) is created, linked to the genai client and the chosen embedding model (models/text-embedding-004). This object will be consistently used for creating embeddings for storage and querying.\nRetry Logic: Robustness is added by wrapping the core generate_content method of the Gemini client with retry logic (google.api_core.retry) to automatically handle transient API errors like rate limits.\nChromaDB Client (chroma_client): A PersistentClient is initialized, pointing to a local directory (./chroma_db). This ensures that any vector collections created will be saved to disk and persist between script runs.","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 3: Initialize Clients (REVISED - Always Init Chroma Locally)\n# ==============================================================================\nprint(\"\\n--- Running Step 3: Initialize Clients (Always Init Chroma Locally) ---\")\n# --- Add Import Here ---\nimport google.generativeai as genai\n# --- End Add Import ---\nimport os\nfrom google.api_core import retry\nimport chromadb\nfrom chromadb.utils import embedding_functions\nimport pandas as pd\n\ngenai_client_initialized = False\ngemini_ef = None\nchroma_client = None\nEMBEDDING_MODEL_NAME=\"models/text-embedding-004\"\n# Use the path determined by config section (which is now always local)\n# Ensure chroma_db_path_to_use exists from config section\nif 'chroma_db_path_to_use' not in locals(): chroma_db_path_to_use = \"./chroma_db\"\n\n# --- 1. Configure Google Generative AI Client ---\nprint(\"Configuring Google Generative AI Client...\")\ntry:\n    api_key = os.environ.get(\"GOOGLE_API_KEY\")\n    if not api_key: raise ValueError(\"GOOGLE_API_KEY environment variable not found.\")\n    genai.configure(api_key=api_key) # Now 'genai' should be defined\n    print(\"Successfully configured Google Generative AI Client.\")\n    genai_client_initialized = True\n\n    # --- Define Embedding Function Consistently ---\n    print(f\"Defining embedding function using model: {EMBEDDING_MODEL_NAME}\")\n    gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(\n        api_key=api_key,\n        model_name=EMBEDDING_MODEL_NAME\n    )\n    print(\"GoogleGenerativeAiEmbeddingFunction defined as 'gemini_ef'.\")\n\n    # --- 2. Add Retry Logic ---\n    print(\"Adding retry logic to genai client...\")\n    is_retriable = lambda e: isinstance(e, genai.types.generation_types.BlockedPromptException) or \\\n                             (hasattr(e, 'message') and 'rate limit' in e.message.lower()) or \\\n                             (hasattr(e, 'code') and e.code in {429, 503}) or \\\n                             isinstance(e, genai.types.generation_types.StopCandidateException)\n    if hasattr(genai, 'GenerativeModel') and hasattr(genai.GenerativeModel, 'generate_content') and not hasattr(genai.GenerativeModel.generate_content, '__wrapped__'):\n           genai.GenerativeModel.generate_content = retry.Retry(predicate=is_retriable)(genai.GenerativeModel.generate_content)\n           print(\"Retry logic applied to GenerativeModel.generate_content method.\")\n    elif not hasattr(genai, 'GenerativeModel'): print(\"Warning: genai.GenerativeModel class not found.\")\n    else: print(\"Retry logic already applied or generate_content method not found.\")\n\nexcept NameError as e: print(f\"NameError during GenAI client setup: {e}. Was 'genai' imported?\") # Specific check\nexcept ValueError as e: print(f\"ValueError during GenAI client setup: {e}\"); genai_client_initialized = False\nexcept AttributeError as e: print(f\"Attribute Error during retry setup: {e}\")\nexcept Exception as e: print(f\"An unexpected error occurred during genai setup: {e}\")\n\n# --- 3. Initialize ChromaDB Client (Using LOCAL Path) ---\nif genai_client_initialized: # Only proceed if GenAI client is OK\n    print(f\"\\nInitializing ChromaDB Persistent Client using LOCAL path: {os.path.abspath(chroma_db_path_to_use)}\")\n    try:\n        # Ensure the local directory exists (important after cleaning)\n        os.makedirs(chroma_db_path_to_use, exist_ok=True)\n        chroma_client = chromadb.PersistentClient(path=chroma_db_path_to_use)\n        print(\"Successfully initialized ChromaDB Persistent Client.\")\n        print(f\"Available collections (in local DB): {chroma_client.list_collections()}\")\n    except Exception as e: print(f\"Error initializing ChromaDB client: {e}\")\n\n# --- Final Status ---\nprint(\"\\n--- Step 3 Final Status ---\")\nif genai_client_initialized and chroma_client is not None and gemini_ef is not None: print(\"Step 3 Completed.\")\nelse: print(\"Step 3 Incomplete.\")\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:42:30.874879Z","iopub.execute_input":"2025-04-27T09:42:30.875191Z","iopub.status.idle":"2025-04-27T09:42:31.256675Z","shell.execute_reply.started":"2025-04-27T09:42:30.875154Z","shell.execute_reply":"2025-04-27T09:42:31.255678Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Step 3: Initialize Clients (Always Init Chroma Locally) ---\nConfiguring Google Generative AI Client...\nSuccessfully configured Google Generative AI Client.\nDefining embedding function using model: models/text-embedding-004\nGoogleGenerativeAiEmbeddingFunction defined as 'gemini_ef'.\nAdding retry logic to genai client...\nRetry logic applied to GenerativeModel.generate_content method.\n\nInitializing ChromaDB Persistent Client using LOCAL path: /kaggle/working/chroma_db\nSuccessfully initialized ChromaDB Persistent Client.\nAvailable collections (in local DB): [Collection(name=country_risk), Collection(name=unspsc_segment_risk), Collection(name=sp_criteria)]\n\n--- Step 3 Final Status ---\nStep 3 Completed.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"Step 4: Connect to Existing SP Criteria Vector Store (Revised Goal)\nIn this revised step, we assume the ChromaDB collection containing the SP criteria embeddings (sp_criteria) has already been created and populated (e.g., in a previous run or separate notebook via the original Step 4 logic). Our goal now is simply to:\nEnsure the chroma_client (initialized in Step 3) is connected to the correct persistent storage directory (./chroma_db).\nRetrieve the existing collection named sp_criteria using the client, ensuring the correct embedding_function (gemini_ef) is associated with the retrieved object for consistency during later queries.\nVerify that the collection was successfully retrieved and check how many items it contains. This confirms access to the pre-populated vector store for use in later steps (like Step 11).\n(Note: The original Step 4 involved populating this collection, similar to Steps 6 & 7 below. That code is available in the history or the step4_original_populate_v2 artifact if needed for initial setup.)\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 4: Populate SP Criteria Collection (REVISED - Added Backup Zip)\n# ==============================================================================\nprint(\"\\n--- Running Step 4: Populate SP Criteria Collection (Added Backup Zip) ---\")\n# Use the code from 'step4_original_populate_v2' immersive, adding zip logic\nimport os\nimport google.generativeai as genai\nimport chromadb\nimport pandas as pd\nfrom chromadb.utils import embedding_functions\nimport time\nimport json\nimport shutil # Import shutil for zipping\n\n# --- Check Prerequisites ---\n# (Prerequisite checks remain the same)\nprerequisites_met_s4_orig = True\nif 'df_criteria' not in locals() or df_criteria is None: print(\"Error (Step 4 Orig): df_criteria DataFrame not found.\"); prerequisites_met_s4_orig = False\nif 'genai_client_initialized' not in locals() or not genai_client_initialized: print(\"Error (Step 4 Orig): GenAI client not initialized.\"); prerequisites_met_s4_orig = False\nif 'chroma_client' not in locals() or chroma_client is None: print(\"Error (Step 4 Orig): ChromaDB client not initialized.\"); prerequisites_met_s4_orig = False\nif 'gemini_ef' not in locals() or gemini_ef is None: print(\"Error (Step 4 Orig): Embedding function 'gemini_ef' not defined.\"); prerequisites_met_s4_orig = False\n\nsp_criteria_collection = None # Initialize collection variable\n\nif prerequisites_met_s4_orig:\n    print(\"Prerequisites met. Proceeding with Original Step 4...\")\n    COLLECTION_NAME_S4 = \"sp_criteria\"\n    BATCH_BACKUP_DIR_S4 = \"./chroma_batch_backups_criteria\"\n    # Use the local path determined by the config section\n    LOCAL_DB_PATH_S4 = chroma_db_path_to_use # Should be \"./chroma_db\"\n    ZIP_BACKUP_FILENAME = \"chroma_db_backup_step4\" # Base name for zip file\n\n    print(f\"Using embedding function 'gemini_ef' defined in Step 3.\")\n    print(f\"\\nPreparing documents, metadatas, and IDs from df_criteria...\")\n    documents_s4, metadatas_s4, ids_s4 = [], [], []\n    preparation_successful_s4, expected_count_s4 = False, 0\n    try:\n        # (Data preparation logic remains the same)\n        text_columns_s4 = ['.', 'UNSPSC', 'SP Category', 'a. Criteria Text', 'b. Required Documentation', 'Market Research', 'Contract clause and KPIs', 'SDG']\n        available_text_columns_s4 = [col for col in text_columns_s4 if col in df_criteria.columns]\n        df_criteria_filled = df_criteria[available_text_columns_s4].fillna('N/A')\n        for index, row in df_criteria_filled.iterrows():\n            doc_parts = [f\"{col}: {row[col]}\" for col in available_text_columns_s4 if col != '.']\n            documents_s4.append(\"\\n\".join(str(part) for part in doc_parts))\n            metadatas_s4.append({'unspsc': str(row.get('UNSPSC', '')), 'category': str(row.get('SP Category', '')), 'sdg': str(row.get('SDG', ''))})\n            ids_s4.append(f\"criteria_{index}\")\n        expected_count_s4 = len(documents_s4); print(f\"Prepared {expected_count_s4} documents.\")\n        if expected_count_s4 > 0: preparation_successful_s4 = True\n        else: print(\"Warning: No documents prepared.\")\n    except Exception as e: print(f\"Error during data preparation: {e}\")\n\n    if preparation_successful_s4:\n        print(f\"\\nGetting or creating ChromaDB collection: '{COLLECTION_NAME_S4}'\")\n        collection_s4 = None # Define collection variable for this scope\n        try:\n            collection_s4 = chroma_client.get_or_create_collection(name=COLLECTION_NAME_S4, embedding_function=gemini_ef)\n            print(f\"Collection '{COLLECTION_NAME_S4}' ready.\")\n            initial_count_s4 = collection_s4.count(); print(f\"Initial count: {initial_count_s4}. Expected count: {expected_count_s4}.\")\n            if initial_count_s4 < expected_count_s4:\n                print(\"Upserting documents...\"); os.makedirs(BATCH_BACKUP_DIR_S4, exist_ok=True); print(f\"Batch backups in: {os.path.abspath(BATCH_BACKUP_DIR_S4)}\")\n                batch_size_s4 = 50; has_error_s4 = False; num_batches_total_s4 = (expected_count_s4 + batch_size_s4 - 1) // batch_size_s4; start_time_upsert_s4 = time.time()\n                for i in range(0, expected_count_s4, batch_size_s4):\n                    # ... (prepare batch) ...\n                    batch_docs = documents_s4[i:i+batch_size_s4]; batch_metadatas = metadatas_s4[i:i+batch_size_s4]; batch_ids = ids_s4[i:i+batch_size_s4]; batch_num = i // batch_size_s4\n                    # ... (save backup optional) ...\n                    print(f\"Attempting to upsert criteria batch {batch_num + 1}/{ num_batches_total_s4 } ({len(batch_ids)} items)...\")\n                    try: collection_s4.upsert(documents=batch_docs, metadatas=batch_metadatas, ids=batch_ids); print(f\" > Batch {batch_num + 1} upserted.\"); time.sleep(1)\n                    except Exception as e: print(f\"ERROR upserting criteria batch {batch_num + 1}: {e}\"); has_error_s4 = True; break\n                end_time_upsert_s4 = time.time(); print(f\"\\nFinished processing criteria batches.\");\n                if not has_error_s4: print(f\"Processed batches in {end_time_upsert_s4 - start_time_upsert_s4:.2f} seconds.\")\n                final_count_s4 = collection_s4.count(); print(f\"Final collection count: {final_count_s4}.\")\n                if not has_error_s4 and final_count_s4 != expected_count_s4: print(f\"Warning: Final count ({final_count_s4}) != expected ({expected_count_s4}).\")\n            else: print(\"Collection already contains expected items. Skipping upsert.\")\n        except Exception as e: print(f\"Error getting/creating/populating collection '{COLLECTION_NAME_S4}': {e}\")\n\n    # --- Immediate Verification and Backup ---\n    print(\"\\n--- Verifying Step 4 Collection ---\")\n    final_status_s4 = \"Incomplete\"\n    if 'collection_s4' in locals() and collection_s4 is not None:\n        try:\n            count_s4 = collection_s4.count(); expected_s4 = expected_count_s4 if 'expected_count_s4' in locals() else -1\n            print(f\"Verification: Collection '{COLLECTION_NAME_S4}' count = {count_s4} (Expected: {expected_s4})\")\n            if count_s4 == expected_s4 and expected_s4 > 0: final_status_s4 = \"Completed Successfully\"\n            elif count_s4 > 0 and 'initial_count_s4' in locals() and initial_count_s4 >= expected_s4 : final_status_s4 = \"Completed (No Action Taken - Already Populated)\"\n            elif count_s4 > 0 : final_status_s4 = f\"Warning - Partially Populated ({count_s4}/{expected_s4})\"\n            else: final_status_s4 = \"Warning - Exists but Empty or Prep Failed\"\n\n            # --- Add Zipping Logic Here ---\n            if final_status_s4.startswith(\"Completed\"):\n                print(f\"\\nAttempting to create zip backup of {LOCAL_DB_PATH_S4}...\")\n                if os.path.isdir(LOCAL_DB_PATH_S4):\n                    try:\n                        zip_output_path = shutil.make_archive(ZIP_BACKUP_FILENAME, 'zip', LOCAL_DB_PATH_S4)\n                        print(f\"Successfully created zip backup: {zip_output_path}\")\n                        print(\"You can download this file from the Output section after the run completes.\")\n                    except Exception as e_zip:\n                        print(f\"ERROR creating zip backup: {e_zip}\")\n                else:\n                    print(f\"ERROR: Cannot create zip backup. Directory not found: {LOCAL_DB_PATH_S4}\")\n            # --- End Zipping Logic ---\n\n        except Exception as e: print(f\"Verification Error: {e}\"); final_status_s4 = \"Verification Error\"\n    else: final_status_s4 = \"Collection object not created/retrieved.\"\n    print(f\"--- Step 4 Final Status: {final_status_s4} ---\")\n    # Assign to global scope if successful\n    if final_status_s4.startswith(\"Completed\") and 'collection_s4' in locals():\n         sp_criteria_collection = collection_s4 # Assign to standard global name\n\nelse: print(\"Original Step 4 Skipped due to missing prerequisites.\")\n# ==============================================================================\n# (End of Revised Step 4 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:42:31.258876Z","iopub.execute_input":"2025-04-27T09:42:31.259156Z","iopub.status.idle":"2025-04-27T09:43:14.627047Z","shell.execute_reply.started":"2025-04-27T09:42:31.259129Z","shell.execute_reply":"2025-04-27T09:43:14.626105Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Step 4: Populate SP Criteria Collection (Added Backup Zip) ---\nPrerequisites met. Proceeding with Original Step 4...\nUsing embedding function 'gemini_ef' defined in Step 3.\n\nPreparing documents, metadatas, and IDs from df_criteria...\nPrepared 59260 documents.\n\nGetting or creating ChromaDB collection: 'sp_criteria'\nCollection 'sp_criteria' ready.\nInitial count: 59260. Expected count: 59260.\nCollection already contains expected items. Skipping upsert.\n\n--- Verifying Step 4 Collection ---\nVerification: Collection 'sp_criteria' count = 59260 (Expected: 59260)\n\nAttempting to create zip backup of ./chroma_db...\nSuccessfully created zip backup: /kaggle/working/chroma_db_backup_step4.zip\nYou can download this file from the Output section after the run completes.\n--- Step 4 Final Status: Completed Successfully ---\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"Just verifying if Step 4 was correctly executed","metadata":{}},{"cell_type":"code","source":"import chromadb\nimport os\nimport pandas as pd # For displaying peek results nicely\n\n# --- Configuration ---\n# Use the SAME path as in Step 3 & 4\nchroma_persist_directory = \"./chroma_db\" # Or the custom path you chose\nCOLLECTION_NAME = \"sp_criteria\"\n\nprint(f\"--- Verifying ChromaDB Collection '{COLLECTION_NAME}' ---\")\n\n# --- 1. Check Storage Location & Connect ---\nstorage_path = os.path.abspath(chroma_persist_directory)\nprint(f\"Attempting to connect to persistent storage at: {storage_path}\")\nif not os.path.exists(storage_path):\n    print(f\"Error: Storage directory does not exist: {storage_path}\")\n    print(\"Ensure Step 3 ran correctly and the path is correct.\")\nelse:\n    print(f\"Storage directory found.\")\n    try:\n        # Connect to the persistent client\n        chroma_client_verify = chromadb.PersistentClient(path=chroma_persist_directory)\n        print(\"Successfully connected to ChromaDB client.\")\n\n        # --- 2. Get Collection & Verify Accessibility ---\n        print(f\"\\nAttempting to retrieve collection: '{COLLECTION_NAME}'\")\n        try:\n            collection = chroma_client_verify.get_collection(name=COLLECTION_NAME)\n            print(f\"Successfully retrieved collection '{COLLECTION_NAME}'. Collection is accessible.\")\n\n            # --- 3. Verify Item Count ---\n            count = collection.count()\n            print(f\"\\nTotal items currently in collection: {count}\")\n            print(\"(Compare this to the total number of documents prepared in Step 4 output)\")\n\n            # --- 4. Peek at Sample Data ---\n            if count > 0:\n                print(f\"\\nRetrieving first 5 items using peek()...\")\n                try:\n                    # Retrieve documents, metadatas, and ids\n                    peek_result = collection.peek(limit=5)\n                    # Display peek results (converting to DataFrame for nice printing)\n                    if peek_result and peek_result.get('ids'): # Check if peek returned data\n                         # Ensure all lists have the same length for DataFrame creation\n                         num_items = len(peek_result['ids'])\n                         docs = peek_result.get('documents', [])\n                         metadatas = peek_result.get('metadatas', [])\n\n                         # Truncate documents if they exist\n                         display_docs = [(doc[:200] + '...' if doc and len(doc) > 200 else doc)\n                                         for doc in docs] if docs else ['N/A'] * num_items\n\n                         # Ensure metadata list matches length\n                         display_metadatas = metadatas if len(metadatas) == num_items else [{}] * num_items\n\n\n                         peek_df = pd.DataFrame({\n                             'ID': peek_result.get('ids', []),\n                             'Metadata': display_metadatas,\n                             'Document (Start)': display_docs\n                         })\n                         try:\n                              print(peek_df.to_markdown(index=False))\n                         except ImportError:\n                              print(peek_df) # Fallback display\n                    else:\n                         print(\"Peek returned no results or unexpected format, although count > 0.\")\n\n                except Exception as e:\n                    print(f\"Error during peek operation: {e}\")\n            else:\n                print(\"Collection is empty, cannot peek.\")\n\n        except ValueError as e:\n            # Handle potential error if collection does not exist (get_collection raises ValueError)\n             print(f\"\\nError: Collection '{COLLECTION_NAME}' does not exist in the database.\")\n             print(\"Step 4 might not have completed successfully or the collection name/path is wrong.\")\n        except Exception as e:\n             print(f\"\\nAn unexpected error occurred retrieving collection: {e}\")\n\n    except Exception as e:\n        print(f\"Error connecting to ChromaDB client at '{storage_path}': {e}\")\n\nprint(\"\\n--- Verification Complete ---\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:14.62863Z","iopub.execute_input":"2025-04-27T09:43:14.629021Z","iopub.status.idle":"2025-04-27T09:43:14.980942Z","shell.execute_reply.started":"2025-04-27T09:43:14.628968Z","shell.execute_reply":"2025-04-27T09:43:14.979898Z"}},"outputs":[{"name":"stdout","text":"--- Verifying ChromaDB Collection 'sp_criteria' ---\nAttempting to connect to persistent storage at: /kaggle/working/chroma_db\nStorage directory found.\nSuccessfully connected to ChromaDB client.\n\nAttempting to retrieve collection: 'sp_criteria'\nSuccessfully retrieved collection 'sp_criteria'. Collection is accessible.\n\nTotal items currently in collection: 59260\n(Compare this to the total number of documents prepared in Step 4 output)\n\nRetrieving first 5 items using peek()...\n| ID         | Metadata                                                                                                                                                                                       | Document (Start)                                                                                                         |\n|:-----------|:-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------|\n| criteria_0 | {'category': 'Labor Standards - Forced labour and human trafficking', 'sdg': 'SDG 8: Decent work and economic growth\\r\\nSDG 12: Responsible consumption and production', 'unspsc': '95141904'} | UNSPSC: 95141904                                                                                                         |\n|            |                                                                                                                                                                                                | SP Category: Labor Standards - Forced labour and human trafficking                                                       |\n|            |                                                                                                                                                                                                | a. Criteria Text: The bidder shall provide documentation that describes the measures taken to avoid, prevent, and er...  |\n| criteria_1 | {'category': 'Health and Safety - Incidents/accidents tracking system', 'sdg': 'SDG 8: Decent work and economic growth', 'unspsc': '95141904'}                                                 | UNSPSC: 95141904                                                                                                         |\n|            |                                                                                                                                                                                                | SP Category: Health and Safety - Incidents/accidents tracking system                                                     |\n|            |                                                                                                                                                                                                | a. Criteria Text: The bidder shall provide documentation that demonstrates how accidents and incidents are tracked...    |\n| criteria_2 | {'category': 'Environmental sustainability - Product life cycle extension', 'sdg': 'SDG 12: Responsible consumption and production\\nSDG 13: Climate action', 'unspsc': '95141904'}             | UNSPSC: 95141904                                                                                                         |\n|            |                                                                                                                                                                                                | SP Category: Environmental sustainability - Product life cycle extension                                                 |\n|            |                                                                                                                                                                                                | a. Criteria Text: A. The bidder shall demonstrate how the useful life of the offered product can be extended t...        |\n| criteria_3 | {'category': 'Environmental sustainability - Sustainable packaging', 'sdg': 'SDG 12: Responsible consumption and production\\nSDG 13: Climate action', 'unspsc': '95141904'}                    | UNSPSC: 95141904                                                                                                         |\n|            |                                                                                                                                                                                                | SP Category: Environmental sustainability - Sustainable packaging                                                        |\n|            |                                                                                                                                                                                                | a. Criteria Text: A. The bidder shall demonstrate that packaging material includes recycled content (e.g. recycled ca... |\n| criteria_4 | {'category': 'Labor Standards - Hiring of local labour', 'sdg': 'SDG 1: No poverty\\nSDG 8: Decent work and economic growth', 'unspsc': '95141904'}                                             | UNSPSC: 95141904                                                                                                         |\n|            |                                                                                                                                                                                                | SP Category: Labor Standards - Hiring of local labour                                                                    |\n|            |                                                                                                                                                                                                | a. Criteria Text: The bidder shall demonstrate that there is minimum local workforce content for direct suppliers        |\n|            |                                                                                                                                                                                                |                                                                                                                          |\n|            |                                                                                                                                                                                                | The bidder sha...                                                                                                        |\n\n--- Verification Complete ---\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"Step 5: Prepare UNSPSC Descriptions Data\nThis step prepares the data needed for the UNSPSC Linking Tool (Step 8). It involves loading official UNSPSC codes and their corresponding text descriptions from a provided file (e.g., unspsc-english-v260801.csv, downloaded via gdown). Crucially, to avoid repeated, costly API calls, this step implements a load-or-generate pattern:\nIt checks if a local file containing pre-computed embeddings (unspsc_embeddings_data_full.pkl) exists.\nIf yes, it loads the codes, descriptions, hierarchy info (Segment, Family, Class), and embeddings directly from this file.\nIf no, it performs a one-time process: loads the source CSV, extracts relevant columns, generates embeddings for all descriptions using the Gemini API (task_type='RETRIEVAL_DOCUMENT'), combines all data, and saves it to the .pkl file for future use.\nThe result is stored in the precomputed_unspsc_data variable.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 5: Prepare UNSPSC Descriptions Data (REVISED - Corrected Signature)\n# ==============================================================================\nprint(\"\\n--- Running Step 5: Prepare UNSPSC Descriptions (Corrected Signature) ---\")\nimport pickle # Add import here for robustness\nimport os\nimport time\nimport gdown\nimport pandas as pd\nimport numpy as np\nimport google.generativeai as genai # Needed for embed_content\n\nprecomputed_unspsc_data = None # Initialize global variable\n# Ensure pkl_path_to_use is defined (from config block)\nif 'pkl_path_to_use' not in locals(): pkl_path_to_use = \"./unspsc_embeddings_data_full.pkl\"\nprecomputed_data_path_s5 = pkl_path_to_use\nunspsc_file_id_s5 = \"1mJFg2b3sKhMH5LmPoaZgXEyDXRp7J7eF\"\nunspsc_local_temp_filename_s5 = \"temp_unspsc_desc_s5.csv\"\n# Ensure embedding model name is defined\nif 'EMBEDDING_MODEL_NAME' not in locals(): EMBEDDING_MODEL_NAME = \"models/text-embedding-004\"\nEMBEDDING_MODEL_NAME_S5 = EMBEDDING_MODEL_NAME\n# Ensure genai_client_initialized flag exists, default to False\nif 'genai_client_initialized' not in locals(): genai_client_initialized = False\n\n# --- Corrected Function Definition ---\ndef load_or_generate_unspsc_data(file_id_or_url, temp_csv_path, save_load_path, embedding_model_name, allow_generation=True): # Added allow_generation parameter\n    \"\"\"Loads data from save_load_path if it exists, otherwise generates and saves.\"\"\"\n    global precomputed_unspsc_data, genai_client_initialized # Access global flag\n    precomputed_unspsc_data = None # Reset before attempting load/gen\n\n    # --- Attempt to load existing file first ---\n    if os.path.exists(save_load_path):\n        print(f\"Attempting to load pre-computed UNSPSC data from: {save_load_path}\")\n        try:\n            with open(save_load_path, 'rb') as f: loaded_data = pickle.load(f) # Uses pickle\n            if isinstance(loaded_data, list) and len(loaded_data) > 0:\n                precomputed_unspsc_data = loaded_data; print(f\"Successfully loaded {len(precomputed_unspsc_data)} entries.\"); return True\n            else: print(\"Warning: Loaded data format invalid. Will attempt generation.\");\n        except Exception as e: print(f\"Warning: Error loading pre-computed data: {e}. Will attempt generation.\");\n\n    # --- Generation logic (if file not found or failed to load) ---\n    # Check if generation is allowed BEFORE proceeding\n    if not allow_generation:\n        print(f\"Pre-computed file not found or failed to load at '{save_load_path}', and generation is disabled (likely because input path was specified but file missing).\")\n        return False\n    print(f\"\\nPre-computed file not found or failed to load at '{save_load_path}'. Generating from source...\")\n\n    # --- Ensure GenAI Client is Initialized HERE ---\n    local_genai_client_initialized = False\n    if genai_client_initialized: # Check global flag set by Step 3\n         print(\"Using globally initialized GenAI client.\")\n         local_genai_client_initialized = True\n    else:\n         print(\"Attempting to initialize GenAI client within Step 5 (Step 3 likely failed)...\")\n         try:\n              api_key = os.environ.get(\"GOOGLE_API_KEY\");\n              if not api_key: raise ValueError(\"GOOGLE_API_KEY needed.\")\n              genai.configure(api_key=api_key)\n              local_genai_client_initialized = True; print(\"GenAI client initialized successfully within Step 5.\")\n              globals()['genai_client_initialized'] = True # Update global flag\n         except Exception as e: print(f\"Error initializing GenAI client within Step 5: {e}\")\n\n    if not local_genai_client_initialized:\n         print(\"Error: GenAI client could not be initialized. Cannot generate embeddings.\")\n         return False\n    # --- End GenAI Client Check ---\n\n    download_successful_s5 = False; df_unspsc = None\n    try: # Download block\n        print(f\"Downloading UNSPSC descriptions CSV using gdown to {temp_csv_path}...\")\n        gdown.download(id=file_id_or_url, output=temp_csv_path, quiet=False)\n        if os.path.exists(temp_csv_path): print(\"Download complete.\"); download_successful_s5 = True\n        else: print(\"ERROR: gdown download failed.\")\n    except Exception as e: print(f\"ERROR: gdown download failed: {e}\")\n    if not download_successful_s5: return False\n\n    try: # Load and Process block\n        print(f\"Loading UNSPSC descriptions from {temp_csv_path}\")\n        # ... (Load CSV logic) ...\n        try: df_unspsc = pd.read_csv(temp_csv_path, low_memory=False)\n        except UnicodeDecodeError: df_unspsc = pd.read_csv(temp_csv_path, encoding='latin-1', low_memory=False)\n        print(f\"CSV loaded. Shape: {df_unspsc.shape}\")\n        # ... (Column definition and check) ...\n        code_col = 'Commodity'; desc_col = 'Commodity Title'; seg_code_col = 'Segment'; seg_title_col = 'Segment Title'; fam_code_col = 'Family'; fam_title_col = 'Family Title'; class_code_col = 'Class'; class_title_col = 'Class Title'\n        required_columns = [code_col, desc_col, seg_code_col, seg_title_col, fam_code_col, fam_title_col, class_code_col, class_title_col]\n        if not all(col in df_unspsc.columns for col in required_columns): print(f\"Error: Missing required columns: {[c for c in required_columns if c not in df_unspsc.columns]}\"); return False\n        # ... (Filtering logic) ...\n        print(\"Filtering out entries with empty descriptions...\"); original_count = len(df_unspsc)\n        df_unspsc_filtered = df_unspsc[df_unspsc[desc_col].fillna('').astype(str).str.strip() != ''].copy()\n        filtered_count = len(df_unspsc_filtered); print(f\"Removed {original_count - filtered_count} entries. Processing {filtered_count} entries.\")\n        if filtered_count == 0: print(\"Error: No entries remaining after filtering.\"); return False\n        # ... (Embedding generation logic) ...\n        print(\"Extracting data...\"); descriptions = df_unspsc_filtered[desc_col].tolist(); print(f\"Extracted {len(descriptions)} descriptions.\")\n        print(f\"\\nGenerating embeddings for {len(descriptions)} descriptions using {embedding_model_name}...\"); start_time = time.time(); embeddings_list = []\n        batch_size_embed = 100 # Gemini API limit per call\n        for i in range(0, len(descriptions), batch_size_embed):\n             batch_desc = descriptions[i:i+batch_size_embed]; print(f\"Embedding batch {i//batch_size_embed + 1}/{(len(descriptions)+batch_size_embed-1)//batch_size_embed}...\")\n             response = genai.embed_content(model=embedding_model_name, content=batch_desc, task_type=\"RETRIEVAL_DOCUMENT\"); embeddings_list.extend(response['embedding']); time.sleep(1) # Add delay\n        end_time = time.time(); print(f\"Generated {len(embeddings_list)} embeddings in {end_time - start_time:.2f} seconds.\")\n        if len(embeddings_list) != len(descriptions): print(\"Error: Mismatch between descriptions and embeddings.\"); return False\n        # ... (Combining data logic) ...\n        print(\"Combining data with embeddings...\")\n        codes = df_unspsc_filtered[code_col].astype(str).tolist(); seg_codes = df_unspsc_filtered[seg_code_col].astype(str).tolist(); seg_titles = df_unspsc_filtered[seg_title_col].fillna('').astype(str).tolist(); fam_codes = df_unspsc_filtered[fam_code_col].astype(str).tolist(); fam_titles = df_unspsc_filtered[fam_title_col].fillna('').astype(str).tolist(); cls_codes = df_unspsc_filtered[class_code_col].astype(str).tolist(); cls_titles = df_unspsc_filtered[class_title_col].fillna('').astype(str).tolist()\n        combined_data = [{'code': c, 'description': d, 'segment_code': sc, 'segment_title': st, 'family_code': fc, 'family_title': ft, 'class_code': cc, 'class_title': ct, 'embedding': e} for c, d, sc, st, fc, ft, cc, ct, e in zip(codes, descriptions, seg_codes, seg_titles, fam_codes, fam_titles, cls_codes, cls_titles, embeddings_list)]\n        # ... (Saving logic) ...\n        # Ensure it saves to the *default* working directory path if generating\n        generation_save_path = \"./unspsc_embeddings_data_full.pkl\"; print(f\"Saving newly generated data to: {generation_save_path}\");\n        os.makedirs(os.path.dirname(generation_save_path) or '.', exist_ok=True)\n        with open(generation_save_path, 'wb') as f: pickle.dump(combined_data, f) # Uses pickle\n        print(\"Data saved successfully.\"); precomputed_unspsc_data = combined_data; return True\n    except Exception as e: print(f\"An error occurred during Step 5 processing: {e}\"); traceback.print_exc(); return False\n    finally:\n        if os.path.exists(temp_csv_path):\n            try: os.remove(temp_csv_path); print(f\"Temporary file '{temp_csv_path}' deleted.\")\n            except Exception as e: print(f\"Warning: Could not delete temp file '{temp_csv_path}'. Error: {e}\")\n\n# Execute loading/generation\n# Determine if generation should be allowed based on whether we are using default path\nshould_generate_s5 = (pkl_path_to_use == \"./unspsc_embeddings_data_full.pkl\")\nload_success_s5 = load_or_generate_unspsc_data(\n    unspsc_file_id_s5, unspsc_local_temp_filename_s5, pkl_path_to_use,\n    EMBEDDING_MODEL_NAME_S5,\n    allow_generation=should_generate_s5 # Pass the flag based on path\n)\n\n# --- Immediate Verification for Step 5 ---\n# (Verification logic remains the same)\nprint(\"\\n--- Verifying Step 5 PKL File ---\")\nfinal_status_s5 = \"Incomplete\"; target_pkl_path = pkl_path_to_use if os.path.exists(pkl_path_to_use) else \"./unspsc_embeddings_data_full.pkl\"\nif 'precomputed_unspsc_data' in locals() and precomputed_unspsc_data is not None and isinstance(precomputed_unspsc_data, list) and len(precomputed_unspsc_data) > 0:\n    data_length = len(precomputed_unspsc_data)\n    if os.path.exists(target_pkl_path): print(f\"Verification: PKL file found at '{target_pkl_path}' and data loaded ({data_length} entries).\"); final_status_s5 = \"Completed Successfully\"\n    else: print(f\"Verification: Data loaded ({data_length} entries), but PKL file NOT found at '{target_pkl_path}'.\"); final_status_s5 = \"Completed (Data In Memory Only)\"\nelif os.path.exists(target_pkl_path): print(f\"Verification: PKL file found at '{target_pkl_path}', but data not loaded.\"); final_status_s5 = \"Failed (Load Error or Empty Data)\"\nelse: print(f\"Verification: PKL file NOT found at '{target_pkl_path}' and data not in memory.\"); final_status_s5 = \"Failed (File Not Found/Generated)\"\nprint(f\"--- Step 5 Final Status: {final_status_s5} ---\")\nif not final_status_s5.startswith(\"Completed\"):\n     if 'prerequisites_ok' in globals(): globals()['prerequisites_ok'] = False\n     else: print(\"Warning: Could not update global prerequisite flag 'prerequisites_ok'.\")\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:14.982323Z","iopub.execute_input":"2025-04-27T09:43:14.982628Z","iopub.status.idle":"2025-04-27T09:43:27.539212Z","shell.execute_reply.started":"2025-04-27T09:43:14.982599Z","shell.execute_reply":"2025-04-27T09:43:27.53814Z"}},"outputs":[{"name":"stdout","text":"\n--- Running Step 5: Prepare UNSPSC Descriptions (Corrected Signature) ---\nAttempting to load pre-computed UNSPSC data from: ./unspsc_embeddings_data_full.pkl\nSuccessfully loaded 149834 entries.\n\n--- Verifying Step 5 PKL File ---\nVerification: PKL file found at './unspsc_embeddings_data_full.pkl' and data loaded (149834 entries).\n--- Step 5 Final Status: Completed Successfully ---\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"Step 6: Prepare UNSPSC Segment Risk Data (Revised Goal)\nThis step creates a vector store focused on sustainability risks associated with UNSPSC Segments, using the DB1 - TO USE.csv file (downloaded via gdown).\nLoad the CSV data.\nExtract relevant columns: Segment (code), Text high risk, Family (code), Position. Assume Segment Title might be missing or combined in the Segment column.\nPrepare documents for embedding by combining the extracted 8-digit Segment code and the Text high risk. Filter out entries with empty risk text.\nPrepare metadata including the cleaned segment_code, family_code, and position.\nCreate/Get the unspsc_segment_risk collection in ChromaDB using the gemini_ef embedding function.\nPopulate the collection using upsert in batches, with delays for robustness. Includes logic to skip if the collection already seems complete based on item count. An immediate verification check confirms the final status.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 6 Code Block - Prepare UNSPSC Segment Risk Data (REVISED - Extract 8-Digit Code)\n# ==============================================================================\nprint(\"\\n--- Defining Step 6: Prepare UNSPSC Segment Risk Data (Revised - Extract 8 Digits) ---\")\n\n# --- Check Prerequisites ---\nprerequisites_met_s6 = True\nif 'chroma_client' not in locals() or chroma_client is None: print(\"Error (Step 6): ChromaDB client not initialized.\"); prerequisites_met_s6 = False\nif 'gemini_ef' not in locals() or gemini_ef is None:\n    print(\"Warning (Step 6): Gemini embedding function 'gemini_ef' not found. Attempting to redefine.\")\n    try:\n        api_key = os.environ.get(\"GOOGLE_API_KEY\");\n        if not api_key: raise ValueError(\"GOOGLE_API_KEY needed.\")\n        EMBEDDING_MODEL_NAME=\"models/text-embedding-004\"\n        gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=api_key, model_name=EMBEDDING_MODEL_NAME, task_type=\"RETRIEVAL_DOCUMENT\")\n        print(\"Redefined Gemini embedding function for Step 6.\")\n    except Exception as e: print(f\"Error: Failed to redefine embedding function: {e}\"); prerequisites_met_s6 = False\n\nif prerequisites_met_s6:\n    print(\"Prerequisites met (ChromaDB client, Embedding Function). Proceeding with Step 6...\")\n\n    # --- Configuration ---\n    unspsc_risk_file_id = \"1XXllP0n1jQ8hJD86piXSEbZAWbyO3vu8\" # File ID for DB1 - TO USE.csv\n    unspsc_risk_local_temp_filename = \"temp_unspsc_risk_s6.csv\"\n\n    # --- Column Names ---\n    segment_code_col = 'Segment' # Column containing potentially combined code/text\n    risk_text_col = 'Text high risk'\n    family_code_col = 'Family' # Keep for metadata\n    position_col = 'Position' # Keep for metadata\n    # segment_title_col = 'Segment Title' # Assume missing based on previous error\n\n    RISK_COLLECTION_NAME_S6 = \"unspsc_segment_risk\" # Use the correct collection name\n\n    # --- 1. Download & Load UNSPSC Risk Data ---\n    df_risk = None\n    download_successful_risk = False\n    # ... (gdown download logic) ...\n    try:\n        print(f\"Downloading UNSPSC Risk CSV using gdown (ID: {unspsc_risk_file_id}) to {unspsc_risk_local_temp_filename}...\")\n        gdown.download(id=unspsc_risk_file_id, output=unspsc_risk_local_temp_filename, quiet=False)\n        if os.path.exists(unspsc_risk_local_temp_filename): print(\"Download complete.\"); download_successful_risk = True\n        else: print(\"ERROR: gdown completed but output file not found.\")\n        if download_successful_risk:\n            print(f\"Loading UNSPSC Risk data from temporary file: {unspsc_risk_local_temp_filename}\")\n            try:\n                df_risk = pd.read_csv(unspsc_risk_local_temp_filename)\n                print(f\"Risk data loaded successfully. Shape: {df_risk.shape}\")\n                print(f\"Columns found: {df_risk.columns.tolist()}\")\n                # Verify required columns exist\n                required_columns = [segment_code_col, risk_text_col, family_code_col, position_col] # Check essential columns\n                if not all(col in df_risk.columns for col in required_columns):\n                     missing = [col for col in required_columns if col not in df_risk.columns]\n                     print(f\"Error: Required columns not found in risk CSV: {missing}\")\n                     df_risk = None\n            except Exception as e: print(f\"Error loading UNSPSC Risk CSV: {e}\"); df_risk = None\n    except Exception as e: print(f\"ERROR: An error occurred during gdown download for risk data: {e}\"); df_risk = None\n    finally:\n        if os.path.exists(unspsc_risk_local_temp_filename):\n            try: os.remove(unspsc_risk_local_temp_filename); print(f\"Temporary file '{unspsc_risk_local_temp_filename}' deleted.\")\n            except Exception as e: print(f\"Warning: Could not delete temp file. Error: {e}\")\n\n    # Proceed only if data loaded successfully\n    if df_risk is not None:\n        # --- 2. Prepare Data for ChromaDB (Extract 8-Digit Segment Code) ---\n        print(f\"\\nPreparing documents, metadatas, and IDs from UNSPSC Segment Risk data...\")\n        segment_risk_documents = []\n        segment_risk_metadatas = []\n        segment_risk_ids = []\n        preparation_successful = False\n        expected_segment_risk_count = 0\n        try:\n            # Filter out rows where risk text is empty/NaN\n            original_count = len(df_risk)\n            df_risk[risk_text_col] = df_risk[risk_text_col].fillna('').astype(str)\n            df_risk_filtered = df_risk[df_risk[risk_text_col].str.strip() != ''].copy()\n            filtered_count = len(df_risk_filtered)\n            print(f\"Removed {original_count - filtered_count} entries with empty '{risk_text_col}'.\")\n            print(f\"Processing {filtered_count} entries.\")\n\n            if filtered_count > 0:\n                processed_count = 0\n                for index, row in df_risk_filtered.iterrows():\n                    # --- Extract 8-digit Segment Code ---\n                    raw_segment = str(row.get(segment_code_col, ''))\n                    segment_code = raw_segment[:8].strip() # Take first 8 chars and strip spaces\n\n                    # Validate format (must be 8 digits)\n                    if not (segment_code.isdigit() and len(segment_code) == 8):\n                        # print(f\"Warning: Extracted segment code '{segment_code}' from '{raw_segment}' is not 8 digits. Skipping row {index}.\")\n                        continue # Skip this row silently now, or keep warning\n                    # --- End Extraction/Validation ---\n\n                    # Combine Segment Code and risk text for the document embedding\n                    risk_text = str(row.get(risk_text_col, ''))\n                    # Use Segment Code for context as Title might be missing or combined\n                    doc_content = f\"Segment Code: {segment_code}\\nHigh Risks: {risk_text}\"\n                    segment_risk_documents.append(doc_content)\n\n                    # Create metadata\n                    try:\n                        position_val = str(row.get(position_col, '')).replace(',', '.')\n                        position_float = float(position_val) if position_val else np.nan\n                    except (ValueError, TypeError): position_float = np.nan\n                    meta = {\n                        'segment_code': segment_code, # Store the cleaned 8-digit code\n                        'family_code': str(row.get(family_code_col, '')),\n                        # Add Segment Title if it exists and is separate under 'Segment Title'\n                        # 'segment_title': str(row.get('Segment Title', ''))\n                    }\n                    if pd.notna(position_float): meta['position'] = position_float\n                    segment_risk_metadatas.append(meta)\n\n                    # Create unique ID using index\n                    segment_risk_ids.append(f\"unspsc_segment_risk_{index}\")\n                    processed_count += 1\n\n                expected_segment_risk_count = len(segment_risk_documents)\n                print(f\"Prepared {expected_segment_risk_count} valid segment risk documents (out of {filtered_count} processed).\")\n                preparation_successful = True\n            else:\n                print(\"Warning: No documents prepared after filtering.\")\n\n        except KeyError as e: print(f\"Error during data preparation: Missing column - {e}.\")\n        except Exception as e: print(f\"Error during data preparation: {e}\")\n\n        # Proceed only if data preparation was successful\n        if preparation_successful and expected_segment_risk_count > 0:\n            # --- 3. Create/Get ChromaDB Segment Risk Collection ---\n            print(f\"\\nGetting or creating ChromaDB collection: '{RISK_COLLECTION_NAME_S6}'\")\n            segment_risk_collection = None\n            try:\n                if 'gemini_ef' not in locals() or gemini_ef is None: raise ValueError(\"Embedding function needed.\")\n                segment_risk_collection = chroma_client.get_or_create_collection(\n                    name=RISK_COLLECTION_NAME_S6, embedding_function=gemini_ef\n                )\n                print(f\"Collection '{RISK_COLLECTION_NAME_S6}' ready.\")\n\n                # --- 4. Add/Upsert Data to Segment Risk Collection ---\n                initial_count = segment_risk_collection.count()\n                print(f\"Initial segment risk collection count: {initial_count}. Expected count: {expected_segment_risk_count}.\")\n                if initial_count < expected_segment_risk_count:\n                    print(\"Segment Risk collection incomplete/empty. Upserting documents...\")\n                    # Using smaller batch size and delay for robustness\n                    batch_size_s6 = 50; has_error_s6 = False; start_time_upsert_s6 = time.time()\n                    num_batches_total_s6 = (expected_segment_risk_count + batch_size_s6 - 1) // batch_size_s6\n                    for i in range(0, expected_segment_risk_count, batch_size_s6):\n                        batch_docs = segment_risk_documents[i:i+batch_size_s6]; batch_metadatas = segment_risk_metadatas[i:i+batch_size_s6]; batch_ids = segment_risk_ids[i:i+batch_size_s6]; batch_num = i // batch_size_s6\n                        print(f\"Attempting to upsert segment risk batch {batch_num + 1}/{ num_batches_total_s6 } ({len(batch_ids)} items)...\")\n                        try:\n                            segment_risk_collection.upsert(documents=batch_docs, metadatas=batch_metadatas, ids=batch_ids)\n                            print(f\" > Batch {batch_num + 1} upserted.\")\n                            time.sleep(1) # Add delay\n                        except Exception as e: print(f\"ERROR upserting batch {batch_num + 1}: {e}\"); has_error_s6 = True; break\n                    end_time_upsert_s6 = time.time(); print(f\"\\nFinished processing segment risk batches.\");\n                    if not has_error_s6: print(f\"Processed batches in {end_time_upsert_s6 - start_time_upsert_s6:.2f}s.\")\n                    final_count = segment_risk_collection.count(); print(f\"Final segment risk collection count: {final_count}.\")\n                    if not has_error_s6 and final_count != expected_segment_risk_count: print(f\"Warning: Final count ({final_count}) doesn't match expected ({expected_segment_risk_count}).\")\n                else: print(\"Segment Risk collection already contains expected items. Skipping upsert.\")\n            except Exception as e: print(f\"Error getting/creating/populating collection '{RISK_COLLECTION_NAME_S6}': {e}\")\n        elif not preparation_successful:\n             print(\"Skipping collection population due to data preparation errors.\")\n        else: # preparation successful but expected_count is 0\n             print(\"Skipping collection population as no valid documents were prepared.\")\n\n\n    # --- Immediate Verification for Step 6 ---\n    print(\"\\n--- Verifying Step 6 Collection ---\")\n    final_status_s6 = \"Incomplete\"\n    if 'segment_risk_collection' in locals() and segment_risk_collection is not None:\n        try:\n            count_s6 = segment_risk_collection.count(); expected_s6 = expected_segment_risk_count if 'expected_segment_risk_count' in locals() else -1\n            print(f\"Verification: Collection '{RISK_COLLECTION_NAME_S6}' count = {count_s6} (Expected: {expected_s6})\")\n            if count_s6 == expected_s6 and expected_s6 > 0: final_status_s6 = \"Completed Successfully\"\n            elif count_s6 > 0 and 'initial_count' in locals() and initial_count >= expected_s6 : final_status_s6 = \"Completed (No Action Taken - Already Populated)\"\n            elif count_s6 > 0 : final_status_s6 = f\"Warning - Partially Populated ({count_s6}/{expected_s6})\"\n            else: final_status_s6 = \"Warning - Exists but Empty or Prep Failed\"\n        except Exception as e: print(f\"Verification Error: {e}\"); final_status_s6 = \"Verification Error\"\n    else: final_status_s6 = \"Collection object not created/retrieved.\"\n    print(f\"--- Step 6 Final Status: {final_status_s6} ---\")\n    # Assign to global scope if successful, needed by later steps if they don't retrieve again\n    if final_status_s6.startswith(\"Completed\") and 'segment_risk_collection' in locals():\n         # Use the correct global variable name consistent with prerequisite checks\n         # segment_risk_collection = segment_risk_collection # Already assigned in local scope\n         pass\n\n\nelse: print(\"Step 6 Skipped due to missing prerequisites.\")\n# ==============================================================================\n# (End of Revised Step 6 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:27.540529Z","iopub.execute_input":"2025-04-27T09:43:27.540827Z","iopub.status.idle":"2025-04-27T09:43:31.251442Z","shell.execute_reply.started":"2025-04-27T09:43:27.540782Z","shell.execute_reply":"2025-04-27T09:43:31.250372Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 6: Prepare UNSPSC Segment Risk Data (Revised - Extract 8 Digits) ---\nPrerequisites met (ChromaDB client, Embedding Function). Proceeding with Step 6...\nDownloading UNSPSC Risk CSV using gdown (ID: 1XXllP0n1jQ8hJD86piXSEbZAWbyO3vu8) to temp_unspsc_risk_s6.csv...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1XXllP0n1jQ8hJD86piXSEbZAWbyO3vu8\nTo: /kaggle/working/temp_unspsc_risk_s6.csv\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 49.1k/49.1k [00:00<00:00, 62.0MB/s]","output_type":"stream"},{"name":"stdout","text":"Download complete.\nLoading UNSPSC Risk data from temporary file: temp_unspsc_risk_s6.csv\nRisk data loaded successfully. Shape: (132, 39)\nColumns found: ['Family', 'Segment', 'Effluents reaching water bodies including ground water', 'Air emissions generated from operations (e.g. volatile organic compounds, aerosols, corrosives, particulates and ozone depleting substances, etc.)', 'Potential use, storage, movement and disposal of environmentally hazardous materials and chemicals', 'Level of CO2 gas emissions throughout the life cycle', 'Level of emissions of other gases with high global warming potential throughout the life cycle', 'Potential waste generated', 'Potential high level of finite materials use throughout the life cycle', 'Use of water', 'Use of land', 'Impacts on biodiversity', 'Impacts on forests', 'Impacts on other natural habitats', \"Potential risks of violating indigenous people's rights (eg. Land grabbing)\", 'Potential Forced Labour Risk', 'Potential Child/Youth Labour risk', 'Potential Working Conditions related risks (eg. Wage and work hours)', 'Health & Safety Risks', 'Potential risks of unequal treatment and contracting terms for women i.e. pay.', 'Potential risks of unequal treatment and contracting terms for different religions i.e. pay.', 'Potential risks of unequal treatment and contracting terms for LGBTQ2IA+ i.e. pay.', 'Potential risks of unequal treatment and contracting terms for minority i.e. pay.', 'Potential risks of unequal treatment and contracting terms for different Races i.e. pay.', 'Potential category-specific sexual harassment and exploitation risk (based on media coverage of the sector/industry)', 'Potential data privacy risk', 'Potential risks concerning product quality/service testing', 'Potential risks related to advertisement, marketing and Intellectual Property (IP)', 'Potential unlawful or harmful use of product/service', 'Unequal access to the services/facilities/products for persons with disabilities', 'Potential Category-specific Corruption Risks', 'Other', 'Score', 'Text high risk', 'Position', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38']\nTemporary file 'temp_unspsc_risk_s6.csv' deleted.\n\nPreparing documents, metadatas, and IDs from UNSPSC Segment Risk data...\nRemoved 0 entries with empty 'Text high risk'.\nProcessing 132 entries.\nPrepared 132 valid segment risk documents (out of 132 processed).\n\nGetting or creating ChromaDB collection: 'unspsc_segment_risk'\nCollection 'unspsc_segment_risk' ready.\nInitial segment risk collection count: 132. Expected count: 132.\nSegment Risk collection already contains expected items. Skipping upsert.\n\n--- Verifying Step 6 Collection ---\nVerification: Collection 'unspsc_segment_risk' count = 132 (Expected: 132)\n--- Step 6 Final Status: Completed Successfully ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"Step 7: Prepare Country-Risk Data\nThis step establishes a knowledge base for country-level sustainability risks using the provided dataset (downloaded via gdown).\nLoad the CSV data.\nExtract relevant columns: Supplier_Country_ISO, Supplier_Country, and the risk flag columns (Child_Labour_Flag, etc.).\nPrepare documents for embedding by creating a text summary for each country listing the risks flagged as 'YES'.\nPrepare metadata including country ISO code, name, and the status ('YES'/'NO') of each individual risk flag.\nCreate/Get the country_risk collection in ChromaDB using the gemini_ef embedding function.\nPopulate the collection using upsert in batches, with delays for robustness, skipping if already complete. An immediate verification check confirms the final status.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 7 Code Block - Prepare Country Risk Data (Flags & gdown - Robust)\n# ==============================================================================\nprint(\"\\n--- Defining Step 7: Prepare Country Risk Data ---\")\n\n# --- Check Prerequisites ---\nprerequisites_met_s7 = True\nif 'chroma_client' not in locals() or chroma_client is None: print(\"Error (Step 7): ChromaDB client not initialized.\"); prerequisites_met_s7 = False\nif 'gemini_ef' not in locals() or gemini_ef is None:\n    print(\"Warning (Step 7): Gemini embedding function 'gemini_ef' not found. Attempting to redefine.\")\n    try:\n        api_key = os.environ.get(\"GOOGLE_API_KEY\");\n        if not api_key: raise ValueError(\"GOOGLE_API_KEY needed.\")\n        EMBEDDING_MODEL_NAME=\"models/text-embedding-004\"\n        gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=api_key, model_name=EMBEDDING_MODEL_NAME, task_type=\"RETRIEVAL_DOCUMENT\")\n        print(\"Redefined Gemini embedding function for Step 7.\")\n    except Exception as e: print(f\"Error: Failed to redefine embedding function: {e}\"); prerequisites_met_s7 = False\n\nif prerequisites_met_s7:\n    print(\"Prerequisites met (ChromaDB client, Embedding Function). Proceeding with Step 7...\")\n\n    # --- Configuration ---\n    # Google Drive File ID for the Country Risk data\n    country_risk_file_id = \"1PzMGgQ0cvs1QIzQul5ZJTfedc_8bzysK\"\n    country_risk_local_temp_filename = \"temp_country_risk_s7.csv\" # Temporary local file name\n\n    # --- Column Names based on user input ---\n    country_iso_col = 'Supplier_Country_ISO'\n    country_name_col = 'Supplier_Country'\n    child_labour_col = 'Child_Labour_Flag'\n    corruption_col = 'Corruption_Flag'\n    forced_labour_col = 'Forced_Labour_Flag'\n    psea_col = 'PSEA_Flag'\n    flag_cols = [child_labour_col, corruption_col, forced_labour_col, psea_col]\n    # --- End Column Names ---\n\n    COUNTRY_RISK_COLLECTION_NAME_S7 = \"country_risk\" # Name for the new ChromaDB collection\n    BATCH_BACKUP_DIR_S7 = \"./chroma_batch_backups_country_risk\" # Optional backup dir\n\n    # --- 1. Download & Load Country Risk Data ---\n    df_country_risk = None\n    download_successful_country = False\n    try:\n        print(f\"Downloading Country Risk CSV using gdown (ID: {country_risk_file_id}) to {country_risk_local_temp_filename}...\")\n        gdown.download(id=country_risk_file_id, output=country_risk_local_temp_filename, quiet=False)\n        if os.path.exists(country_risk_local_temp_filename): print(\"Download complete.\"); download_successful_country = True\n        else: print(\"ERROR: gdown completed but output file not found.\")\n\n        if download_successful_country:\n            print(f\"Loading Country Risk data from temporary file: {country_risk_local_temp_filename}\")\n            try:\n                df_country_risk = pd.read_csv(country_risk_local_temp_filename)\n                print(f\"Country Risk data loaded successfully. Shape: {df_country_risk.shape}\")\n                print(f\"Columns found: {df_country_risk.columns.tolist()}\")\n                required_columns = [country_iso_col, country_name_col] + flag_cols\n                if not all(col in df_country_risk.columns for col in required_columns):\n                     missing = [col for col in required_columns if col not in df_country_risk.columns]\n                     print(f\"Error: Required columns not found in Country Risk CSV: {missing}\")\n                     df_country_risk = None\n            except Exception as e: print(f\"Error loading Country Risk CSV from temp file: {e}\"); df_country_risk = None\n    except Exception as e: print(f\"ERROR: An error occurred during gdown download for country risk data: {e}\"); df_country_risk = None\n    finally:\n        if os.path.exists(country_risk_local_temp_filename):\n            try: os.remove(country_risk_local_temp_filename); print(f\"Temporary file '{country_risk_local_temp_filename}' deleted.\")\n            except Exception as e: print(f\"Warning: Could not delete temporary file '{country_risk_local_temp_filename}'. Error: {e}\")\n\n    # Proceed only if data loaded successfully\n    if df_country_risk is not None:\n        # --- 2. Prepare Data for ChromaDB ---\n        print(f\"\\nPreparing documents, metadatas, and IDs from Country Risk data...\")\n        country_documents = []\n        country_metadatas = []\n        country_ids = []\n        preparation_successful = False\n        expected_country_count = 0\n        try:\n            # Ensure flag columns are strings and handle NaN before comparison\n            for col in flag_cols:\n                 df_country_risk[col] = df_country_risk[col].fillna('NO').astype(str)\n\n            # Optional: Drop duplicates based on country identifier if needed\n            # df_country_risk.drop_duplicates(subset=[country_iso_col], keep='first', inplace=True)\n            # print(f\"Shape after potential drop_duplicates: {df_country_risk.shape}\")\n\n            # Iterate through the DataFrame rows\n            for index, row in df_country_risk.iterrows():\n                country_name = str(row.get(country_name_col, '')).strip()\n                country_iso = str(row.get(country_iso_col, '')).strip()\n\n                # Skip if essential identifiers are missing\n                if not country_name or not country_iso:\n                     print(f\"Skipping row {index} due to missing country name or ISO code.\")\n                     continue\n\n                # Identify active risks based on 'YES' flags (case-insensitive)\n                active_risks = []\n                flag_status = {}\n                cl_flag = row[child_labour_col].strip().upper(); flag_status['child_labour'] = cl_flag\n                if cl_flag == 'YES': active_risks.append(\"Child Labour\")\n                co_flag = row[corruption_col].strip().upper(); flag_status['corruption'] = co_flag\n                if co_flag == 'YES': active_risks.append(\"Corruption\")\n                fl_flag = row[forced_labour_col].strip().upper(); flag_status['forced_labour'] = fl_flag\n                if fl_flag == 'YES': active_risks.append(\"Forced Labour\")\n                ps_flag = row[psea_col].strip().upper(); flag_status['psea'] = ps_flag\n                if ps_flag == 'YES': active_risks.append(\"PSEA\")\n\n                # Create document content\n                if active_risks: risk_summary = \", \".join(active_risks)\n                else: risk_summary = \"None Reported\"\n                doc_content = f\"Country: {country_name} ({country_iso})\\nRisks Present: {risk_summary}\"\n                country_documents.append(doc_content)\n\n                # Create metadata including flags\n                meta = {'country_iso': country_iso, 'country_name': country_name, **flag_status}\n                country_metadatas.append(meta)\n\n                # Create unique ID using index or ISO code if unique\n                country_ids.append(f\"country_risk_{country_iso}_{index}\") # Make ID more specific\n\n            expected_country_count = len(country_documents)\n            print(f\"Prepared {expected_country_count} country risk documents.\")\n            preparation_successful = True\n\n        except KeyError as e: print(f\"Error during data preparation: Missing expected column - {e}.\")\n        except Exception as e: print(f\"Error during data preparation: {e}\")\n\n        # Proceed only if data preparation was successful\n        if preparation_successful and expected_country_count > 0:\n            # --- 3. Create/Get ChromaDB Country Risk Collection ---\n            print(f\"\\nGetting or creating ChromaDB collection: '{COUNTRY_RISK_COLLECTION_NAME_S7}'\")\n            country_risk_collection = None\n            try:\n                if 'gemini_ef' not in locals() or gemini_ef is None: raise ValueError(\"Embedding function needed.\")\n                country_risk_collection = chroma_client.get_or_create_collection(\n                    name=COUNTRY_RISK_COLLECTION_NAME_S7, embedding_function=gemini_ef\n                )\n                print(f\"Collection '{COUNTRY_RISK_COLLECTION_NAME_S7}' ready.\")\n\n                # --- 4. Add/Upsert Data to Country Risk Collection ---\n                initial_country_risk_count = country_risk_collection.count()\n                print(f\"Initial country risk collection count: {initial_country_risk_count}. Expected count: {expected_country_count}.\")\n                if initial_country_risk_count < expected_country_count:\n                    print(\"Country Risk collection incomplete/empty. Upserting documents...\")\n                    os.makedirs(BATCH_BACKUP_DIR_S7, exist_ok=True) # Optional backup\n                    batch_size_s7 = 50; has_error_s7 = False; start_time_upsert_s7 = time.time()\n                    num_batches_total_s7 = (expected_country_count + batch_size_s7 - 1) // batch_size_s7\n                    for i in range(0, expected_country_count, batch_size_s7):\n                        batch_docs = country_documents[i:i+batch_size_s7]; batch_metadatas = country_metadatas[i:i+batch_size_s7]; batch_ids = country_ids[i:i+batch_size_s7]; batch_num = i // batch_size_s7\n                        # Optional: Save backup\n                        # backup_filename = os.path.join(BATCH_BACKUP_DIR_S7, f\"batch_{batch_num}.json\"); #... save json ...\n                        print(f\"Attempting to upsert country risk batch {batch_num + 1}/{ num_batches_total_s7 } ({len(batch_ids)} items)...\")\n                        try:\n                            country_risk_collection.upsert(documents=batch_docs, metadatas=batch_metadatas, ids=batch_ids)\n                            print(f\" > Batch {batch_num + 1} upserted.\")\n                            time.sleep(1) # Add delay\n                        except Exception as e: print(f\"ERROR upserting country risk batch {batch_num + 1}: {e}\"); has_error_s7 = True; break\n                    end_time_upsert_s7 = time.time(); print(f\"\\nFinished processing country risk batches.\");\n                    if not has_error_s7: print(f\"Processed batches in {end_time_upsert_s7 - start_time_upsert_s7:.2f}s.\")\n                    final_count = country_risk_collection.count(); print(f\"Final country risk collection count: {final_count}.\")\n                    if not has_error_s7 and final_count != expected_country_count: print(f\"Warning: Final count ({final_count}) doesn't match expected ({expected_country_count}).\")\n                else: print(\"Country Risk collection already contains expected items. Skipping upsert.\")\n            except Exception as e: print(f\"Error getting/creating/populating collection '{COUNTRY_RISK_COLLECTION_NAME_S7}': {e}\")\n        elif not preparation_successful: print(\"Skipping collection population due to data preparation errors.\")\n        else: print(\"Skipping collection population as no valid documents were prepared.\")\n\n    # --- Immediate Verification for Step 7 ---\n    print(\"\\n--- Verifying Step 7 Collection ---\")\n    final_status_s7 = \"Incomplete\"\n    if 'country_risk_collection' in locals() and country_risk_collection is not None:\n        try:\n            count_s7 = country_risk_collection.count(); expected_s7 = expected_country_count if 'expected_country_count' in locals() else -1\n            print(f\"Verification: Collection '{COUNTRY_RISK_COLLECTION_NAME_S7}' count = {count_s7} (Expected: {expected_s7})\")\n            if count_s7 == expected_s7 and expected_s7 > 0: final_status_s7 = \"Completed Successfully\"\n            elif count_s7 > 0 and 'initial_country_risk_count' in locals() and initial_country_risk_count >= expected_s7 : final_status_s7 = \"Completed (No Action Taken - Already Populated)\"\n            elif count_s7 > 0 : final_status_s7 = f\"Warning - Partially Populated ({count_s7}/{expected_s7})\"\n            else: final_status_s7 = \"Warning - Exists but Empty or Prep Failed\"\n        except Exception as e: print(f\"Verification Error: {e}\"); final_status_s7 = \"Verification Error\"\n    else: final_status_s7 = \"Collection object not created/retrieved.\"\n    print(f\"--- Step 7 Final Status: {final_status_s7} ---\")\n    # Assign to global scope if successful\n    if final_status_s7.startswith(\"Completed\") and 'country_risk_collection' in locals():\n         # country_risk_collection = country_risk_collection # Already assigned\n         pass\n\nelse: print(\"Step 7 Skipped due to missing prerequisites.\")\n# ==============================================================================\n# (End of Revised Step 7 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:31.252653Z","iopub.execute_input":"2025-04-27T09:43:31.253007Z","iopub.status.idle":"2025-04-27T09:43:35.04466Z","shell.execute_reply.started":"2025-04-27T09:43:31.252972Z","shell.execute_reply":"2025-04-27T09:43:35.043764Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 7: Prepare Country Risk Data ---\nPrerequisites met (ChromaDB client, Embedding Function). Proceeding with Step 7...\nDownloading Country Risk CSV using gdown (ID: 1PzMGgQ0cvs1QIzQul5ZJTfedc_8bzysK) to temp_country_risk_s7.csv...\n","output_type":"stream"},{"name":"stderr","text":"Downloading...\nFrom: https://drive.google.com/uc?id=1PzMGgQ0cvs1QIzQul5ZJTfedc_8bzysK\nTo: /kaggle/working/temp_country_risk_s7.csv\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.29k/7.29k [00:00<00:00, 8.00MB/s]","output_type":"stream"},{"name":"stdout","text":"Download complete.\nLoading Country Risk data from temporary file: temp_country_risk_s7.csv\nCountry Risk data loaded successfully. Shape: (210, 7)\nColumns found: ['Supplier_Country_ISO', 'Supplier_Country', 'Year', 'Child_Labour_Flag', 'Corruption_Flag', 'Forced_Labour_Flag', 'PSEA_Flag']\nTemporary file 'temp_country_risk_s7.csv' deleted.\n\nPreparing documents, metadatas, and IDs from Country Risk data...\nPrepared 210 country risk documents.\n\nGetting or creating ChromaDB collection: 'country_risk'\nCollection 'country_risk' ready.\nInitial country risk collection count: 210. Expected count: 210.\nCountry Risk collection already contains expected items. Skipping upsert.\n\n--- Verifying Step 7 Collection ---\nVerification: Collection 'country_risk' count = 210 (Expected: 210)\n--- Step 7 Final Status: Completed Successfully ---\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"Step 8: Implement UNSPSC Linking Tool\nThis step defines the first functional tool: find_similar_unspsc.\nInput: Takes a natural language item description.\nQuery Embedding: Generates a RETRIEVAL_QUERY embedding for the input description using the Gemini embedding model.\nLoad Data: Accesses the precomputed_unspsc_data (from Step 5).\nSimilarity Search: Calculates cosine similarity between the query embedding and all pre-computed UNSPSC description embeddings.\nOutput: Returns the details (code, description, hierarchy info, similarity score) of the top N closest matching UNSPSC entries.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 8 Code Block - UNSPSC Linking Tool Function\n# ==============================================================================\nprint(\"\\n--- Defining Step 8: Implement UNSPSC Linking Tool ---\")\n\n# --- Check Prerequisites from previous steps ---\nprerequisites_met_s8 = True\n# Need genai client from Step 3\nif 'genai_client_initialized' not in locals() or not genai_client_initialized:\n    print(\"Error (Step 8): Google Generative AI Client not initialized.\")\n    prerequisites_met_s8 = False\n# Need precomputed UNSPSC data from Step 5\nif 'precomputed_unspsc_data' not in locals() or precomputed_unspsc_data is None:\n    # Attempt to load it if the variable doesn't exist (e.g., kernel restart)\n    precomputed_data_path = \"unspsc_embeddings_data_full.pkl\" # Path from Step 5\n    if os.path.exists(precomputed_data_path):\n         print(f\"Attempting to load pre-computed UNSPSC data from: {precomputed_data_path}\")\n         try:\n              with open(precomputed_data_path, 'rb') as f:\n                   precomputed_unspsc_data = pickle.load(f)\n              if not (isinstance(precomputed_unspsc_data, list) and len(precomputed_unspsc_data) > 0 and isinstance(precomputed_unspsc_data[0], dict)):\n                   print(\"Error: Loaded precomputed data has unexpected format.\")\n                   precomputed_unspsc_data = None; prerequisites_met_s8 = False\n              else: print(f\"Successfully loaded {len(precomputed_unspsc_data)} pre-computed UNSPSC entries.\")\n         except Exception as e: print(f\"Error loading pre-computed data: {e}\"); precomputed_unspsc_data = None; prerequisites_met_s8 = False\n    else:\n        print(f\"Error: precomputed_unspsc_data not found in memory and file not found: {precomputed_data_path}\")\n        print(f\"Please ensure Step 5 ran successfully and '{precomputed_data_path}' exists.\")\n        prerequisites_met_s8 = False\n\n# --- Configuration ---\nEMBEDDING_MODEL_NAME_S8 = \"models/text-embedding-004\" # Ensure consistency with Step 5 embedding\n\n# --- UNSPSC Linking Function ---\ndef find_similar_unspsc(item_description: str, unspsc_data: list, top_n: int = 1) -> list:\n    \"\"\"\n    Finds the top_n most semantically similar UNSPSC entries for a given item description.\n\n    Args:\n        item_description: The user's text description of the item.\n        unspsc_data: The list of dictionaries loaded/generated in Step 5,\n                     containing 'code', 'description', 'embedding', hierarchy info, etc.\n        top_n: The number of top matches to return.\n\n    Returns:\n        A list of dictionaries, each containing details of a match\n        (code, description, similarity_score, segment_code, etc.),\n        sorted by similarity score descending. Returns empty list on error.\n    \"\"\"\n    if not prerequisites_met_s8: # Check if prerequisites were met when function was defined\n         print(\"Error: Prerequisites (GenAI client, UNSPSC data) not met. Cannot perform search.\")\n         return []\n    if not unspsc_data:\n        print(\"Error: UNSPSC data not available for searching.\")\n        return []\n    if not item_description or not item_description.strip():\n         print(\"Error: Item description cannot be empty.\")\n         return []\n\n    print(f\"\\n--- Finding UNSPSC match for: '{item_description}' ---\")\n    try:\n        # 1. Generate Query Embedding\n        print(f\"Generating query embedding using {EMBEDDING_MODEL_NAME_S8}...\")\n        # Ensure genai client is configured before calling embed_content\n        if 'genai_client_initialized' not in globals() or not genai_client_initialized:\n             raise RuntimeError(\"GenAI Client not initialized.\")\n\n        query_response = genai.embed_content(\n            model=EMBEDDING_MODEL_NAME_S8,\n            content=item_description,\n            task_type=\"RETRIEVAL_QUERY\" # Use QUERY type for searching\n        )\n        query_embedding = query_response['embedding']\n        print(\"Query embedding generated.\")\n\n        # 2. Extract Corpus Embeddings and Data\n        # Ensure data format is correct before extracting\n        if not isinstance(unspsc_data[0].get('embedding'), list):\n             print(\"Error: Precomputed data does not contain valid embeddings.\")\n             return []\n        corpus_embeddings = np.array([item['embedding'] for item in unspsc_data])\n        # Ensure query_embedding is 2D for cosine_similarity\n        query_embedding_2d = np.array(query_embedding).reshape(1, -1)\n\n        # 3. Calculate Similarities\n        print(\"Calculating similarities...\")\n        similarities = cosine_similarity(query_embedding_2d, corpus_embeddings)[0] # Get the 1D array of scores\n\n        # 4. Find Top N Matches\n        # Ensure top_n doesn't exceed the number of available items\n        actual_top_n = min(top_n, len(similarities))\n        if actual_top_n <= 0: print(\"Warning: No similarities calculated or data empty.\"); return []\n        # Get indices of top N scores in descending order\n        top_n_indices = np.argsort(similarities)[-actual_top_n:][::-1]\n\n        # 5. Format Results\n        results = []\n        print(f\"Top {actual_top_n} matches:\")\n        for i in top_n_indices:\n            match_data = unspsc_data[i]\n            score = similarities[i]\n            result = {\n                'code': match_data.get('code'),\n                'description': match_data.get('description'),\n                'similarity_score': float(score), # Convert numpy float if necessary\n                'segment_code': match_data.get('segment_code'),\n                'segment_title': match_data.get('segment_title'), # Include if available\n                'family_code': match_data.get('family_code'),\n                'family_title': match_data.get('family_title'), # Include if available\n                'class_code': match_data.get('class_code'),\n                'class_title': match_data.get('class_title') # Include if available\n            }\n            results.append(result)\n            print(f\"  - Score: {score:.4f}, Code: {result['code']}, Desc: {result['description'][:80]}...\")\n\n        return results\n\n    except Exception as e:\n        print(f\"An error occurred during UNSPSC linking: {e}\")\n        # import traceback\n        # traceback.print_exc()\n        return []\n\n# --- Example Usage (Optional - Uncomment to test) ---\nif prerequisites_met_s8 and 'precomputed_unspsc_data' in locals() and precomputed_unspsc_data is not None:\n    print(\"\\n--- Running Example UNSPSC Linking ---\")\n    test_description_1 = \"Desktop computers for office use\"\n    test_description_2 = \"Heavy duty generators for field hospital\"\n    test_description_3 = \"Sustainable coffee beans\"\n\n    # print(f\"\\n--- Testing: '{test_description_1}' ---\")\n    # matches_1 = find_similar_unspsc(test_description_1, precomputed_unspsc_data, top_n=1)\n    # print(f\"Result: {matches_1}\")\n\n    # print(f\"\\n--- Testing: '{test_description_2}' ---\")\n    # matches_2 = find_similar_unspsc(test_description_2, precomputed_unspsc_data, top_n=3)\n    # print(f\"Result: {matches_2}\")\n\n    # print(f\"\\n--- Testing: '{test_description_3}' ---\")\n    # matches_3 = find_similar_unspsc(test_description_3, precomputed_unspsc_data, top_n=1)\n    # print(f\"Result: {matches_3}\")\n    print(\"\\n--- Example Usage Complete (results printed above if uncommented) ---\")\nelse:\n     print(\"\\nSkipping example usage due to missing prerequisites.\")\n\n\n# --- Final Status ---\nprint(\"\\n--- Step 8 Final Status ---\")\nif prerequisites_met_s8 and 'find_similar_unspsc' in locals():\n    print(\"Step 8 Completed: Function 'find_similar_unspsc' is defined.\")\n    print(\"You can now call this function with an item description.\")\nelse:\n    print(\"Step 8 Incomplete: Failed to define function or prerequisites missing.\")\n\n# ==============================================================================\n# (End of Step 8 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.045679Z","iopub.execute_input":"2025-04-27T09:43:35.045948Z","iopub.status.idle":"2025-04-27T09:43:35.063465Z","shell.execute_reply.started":"2025-04-27T09:43:35.045925Z","shell.execute_reply":"2025-04-27T09:43:35.062397Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 8: Implement UNSPSC Linking Tool ---\n\n--- Running Example UNSPSC Linking ---\n\n--- Example Usage Complete (results printed above if uncommented) ---\n\n--- Step 8 Final Status ---\nStep 8 Completed: Function 'find_similar_unspsc' is defined.\nYou can now call this function with an item description.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"Step 9: Implement Risk Identification Tool (Revised Goal)\nThis step defines the identify_risks tool function (revised).\nInput: Takes a UNSPSC Segment code (string, expected 8 digits) and a country identifier (string, name or ISO code).\nConnect: Gets handles to the unspsc_segment_risk and country_risk collections.\nQuery: Performs direct lookups (metadata filtering using collection.get(where=...)) in both collections based on the provided segment code and country identifier.\nOutput: Returns a dictionary containing the retrieved documents and metadata (summarized risks and flags) for the matching segment and country, or indicates if no entries were found. Includes input validation for the segment code format.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 9 Code Block - Risk Identification Tool Function (REVISED)\n# ==============================================================================\nprint(\"\\n--- Defining Step 9: Implement Risk Identification Tool (Revised) ---\")\n# --- Check Prerequisites ---\nprerequisites_met_s9 = True\nif 'chroma_client' not in locals() or chroma_client is None: print(\"Error (Step 9): ChromaDB client not initialized.\"); prerequisites_met_s9 = False\n\n# --- Define Collection Names (Revised) ---\nSEGMENT_RISK_COLLECTION_NAME = \"unspsc_segment_risk\" # Use new name\nCOUNTRY_RISK_COLLECTION_NAME = \"country_risk\" # Keep country risk name\n\n# --- Risk Identification Function (Revised Signature and Logic) ---\ndef identify_risks(segment_code: str, country_identifier: str, search_by_iso: bool = True) -> dict:\n    \"\"\"\n    Identifies sustainability risks from ChromaDB collections based on UNSPSC Segment code and country.\n\n    Args:\n        segment_code: The UNSPSC Segment code (string) to search for risks.\n        country_identifier: The country name (string) or ISO code (string) to search for risks.\n        search_by_iso: If True, assumes country_identifier is an ISO code; otherwise uses name.\n\n    Returns:\n        A dictionary containing identified risks for segment and country.\n        Returns None if prerequisites are not met.\n    \"\"\"\n    # Use the global chroma_client defined in Step 3\n    global chroma_client\n    if not prerequisites_met_s9 or chroma_client is None:\n        print(\"Error: Prerequisites (ChromaDB client) not met for identify_risks.\")\n        return {\"errors\": [\"Prerequisites not met.\"]} # Return error dict\n\n    print(f\"\\n--- Identifying risks for Segment Code: '{segment_code}' and Country: '{country_identifier}' ---\")\n    results = {\n        'segment_code': segment_code,\n        'segment_risks': {'count': 0, 'documents': [], 'metadatas': []}, # Changed key\n        'country_identifier': country_identifier,\n        'country_risks': {'count': 0, 'documents': [], 'metadatas': []},\n        'errors': []\n    }\n    segment_risk_collection = None # Changed variable name\n    country_risk_collection = None\n\n    try:\n        # 1. Get Collections (Ensure client is valid)\n        print(f\"Attempting to get collection: '{SEGMENT_RISK_COLLECTION_NAME}'\")\n        segment_risk_collection = chroma_client.get_collection(name=SEGMENT_RISK_COLLECTION_NAME)\n        print(f\"Successfully retrieved collection '{SEGMENT_RISK_COLLECTION_NAME}'.\")\n\n        print(f\"Attempting to get collection: '{COUNTRY_RISK_COLLECTION_NAME}'\")\n        country_risk_collection = chroma_client.get_collection(name=COUNTRY_RISK_COLLECTION_NAME)\n        print(f\"Successfully retrieved collection '{COUNTRY_RISK_COLLECTION_NAME}'.\")\n\n    except Exception as e:\n         error_msg = f\"Error: Failed to retrieve one or both risk collections in identify_risks. Did Steps 6 (Revised) & 7 complete? Details: {e}\"\n         print(error_msg); results['errors'].append(error_msg); return results\n\n    # Proceed if collections were retrieved\n    if segment_risk_collection:\n        try:\n            # 2. Query UNSPSC Segment Risks using metadata filter\n            print(f\"\\nQuerying UNSPSC Segment risks for Segment Code '{segment_code}'...\")\n            segment_results = segment_risk_collection.get(\n                where={'segment_code': str(segment_code)}, # Filter by segment_code\n                include=['metadatas', 'documents']\n            )\n            if segment_results and segment_results.get('ids'):\n                count = len(segment_results['ids'])\n                print(f\"Found {count} risk entries for Segment Code '{segment_code}'.\")\n                results['segment_risks']['count'] = count # Changed key\n                results['segment_risks']['documents'] = segment_results.get('documents', [])\n                results['segment_risks']['metadatas'] = segment_results.get('metadatas', [])\n            else:\n                print(f\"No risk entries found for Segment Code '{segment_code}'.\")\n        except Exception as e:\n             error_msg = f\"Error querying collection '{SEGMENT_RISK_COLLECTION_NAME}': {e}\"\n             print(error_msg); results['errors'].append(error_msg)\n\n    if country_risk_collection:\n        try:\n            # 3. Query Country Risks (remains the same)\n            print(f\"\\nQuerying Country risks for '{country_identifier}'...\")\n            country_filter_key = 'country_iso' if search_by_iso else 'country_name'\n            country_results = country_risk_collection.get(\n                where={country_filter_key: str(country_identifier)},\n                include=['metadatas', 'documents']\n            )\n            if country_results and country_results.get('ids'):\n                count = len(country_results['ids'])\n                print(f\"Found {count} risk entries for Country '{country_identifier}'.\")\n                results['country_risks']['count'] = count\n                results['country_risks']['documents'] = country_results.get('documents', [])\n                results['country_risks']['metadatas'] = country_results.get('metadatas', [])\n            else:\n                print(f\"No risk entries found for Country '{country_identifier}'.\")\n        except Exception as e:\n             error_msg = f\"Error querying collection '{COUNTRY_RISK_COLLECTION_NAME}': {e}\"\n             print(error_msg); results['errors'].append(error_msg)\n\n    print(\"\\n--- Risk identification complete ---\")\n    return results\n\nif 'identify_risks' in globals():\n     print(\"Step 9 function 'identify_risks' (re)defined with segment_code.\")\n# ==============================================================================\n# (End of Step 9 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.065991Z","iopub.execute_input":"2025-04-27T09:43:35.066282Z","iopub.status.idle":"2025-04-27T09:43:35.088564Z","shell.execute_reply.started":"2025-04-27T09:43:35.06625Z","shell.execute_reply":"2025-04-27T09:43:35.087293Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 9: Implement Risk Identification Tool (Revised) ---\nStep 9 function 'identify_risks' (re)defined with segment_code.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"Step 10: Implement Contextual Research Tool\nThis step defines the research_context_with_grounding tool function.\nInput: Takes a natural language search query string.\nConfigure Search Tool: Configures the Gemini API call to enable the GoogleSearchRetrieval tool.\nExecute Grounded Query: Sends the query to a Gemini model (e.g., gemini-1.5-flash-latest) with the tool enabled.\nOutput: Returns the model's text response, which incorporates information grounded by the Google Search results.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 10: Implement Contextual Research Tool (REVISED - Simplified v4 - Implicit Grounding)\n# ==============================================================================\nprint(\"\\n--- Defining Step 10: Implement Contextual Research Tool (Simplified - Implicit Grounding) ---\")\n\nprerequisites_met_s10 = True\nif 'genai_client_initialized' not in locals() or not genai_client_initialized: print(\"Error (Step 10): GenAI client not initialized.\"); prerequisites_met_s10 = False\n\ndef research_context_with_grounding(search_query: str) -> str | None:\n    \"\"\"\n    Performs a search query using the Gemini model, relying on its\n    built-in grounding capabilities by calling model.generate_content directly.\n    \"\"\"\n    if not prerequisites_met_s10:\n        print(\"Error: Prerequisites not met for research_context_with_grounding.\"); return None\n    if not search_query or not search_query.strip():\n        print(\"Error: Search query cannot be empty.\"); return None\n\n    print(f\"\\n--- Performing grounded search for: '{search_query}' ---\")\n    try:\n        # --- Instantiate Model ---\n        model_name_search = 'gemini-1.5-flash-latest'; print(f\"Using model: {model_name_search}\")\n        if not genai_client_initialized: raise RuntimeError(\"GenAI client not initialized\")\n        model = genai.GenerativeModel(model_name_search)\n\n        # --- Call generate_content directly (NO explicit tools/config) ---\n        print(f\"Calling model.generate_content (relying on implicit grounding)...\")\n        # We can still pass safety settings if desired for this specific call\n        # safety_settings_s10 = {...} # Define if needed\n        response = model.generate_content(\n            search_query\n            # generation_config=... # Can add basic config (e.g., temp) if needed\n            # safety_settings=safety_settings_s10\n        )\n        # --- End Call ---\n\n        print(\"Search query response received.\")\n        # --- Process Response ---\n        if response and hasattr(response, 'text'):\n             return response.text\n        elif response and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n             print(f\"Warning: Prompt was blocked. Reason: {response.prompt_feedback.block_reason}\")\n             return f\"Error: Search blocked. Reason: {response.prompt_feedback.block_reason}\"\n        else:\n             print(\"Warning: Response received, but no text content found or unexpected format.\")\n             # print(f\"Full response: {response}\") # Debugging\n             return \"Error: No text content in response.\"\n        # --- End Process Response ---\n\n    except Exception as e:\n        print(f\"An error occurred during grounded search: {e}\")\n        # import traceback # Uncomment for detailed traceback\n        # traceback.print_exc()\n        return f\"Error: {e}\"\n\nif 'research_context_with_grounding' in globals(): print(\"Step 10 function 'research_context_with_grounding' defined (simplified - implicit grounding).\")\n# ==============================================================================\n# (End of Step 10 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.089777Z","iopub.execute_input":"2025-04-27T09:43:35.090135Z","iopub.status.idle":"2025-04-27T09:43:35.113895Z","shell.execute_reply.started":"2025-04-27T09:43:35.090108Z","shell.execute_reply":"2025-04-27T09:43:35.112827Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 10: Implement Contextual Research Tool (Simplified - Implicit Grounding) ---\nStep 10 function 'research_context_with_grounding' defined (simplified - implicit grounding).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"Step 11: Implement SP Criteria Suggestion Tool (Revised Goal)\nThis step defines the suggest_sp_criteria tool function (revised).\nInput: Takes an item description and an optional UNSPSC code (Commodity code).\nConnect: Gets a handle to the sp_criteria collection.\nExplicit Query Embedding: Explicitly generates a RETRIEVAL_QUERY embedding for the input description using the Gemini embedding model (models/text-embedding-004).\nQuery: Queries the collection using the generated query_embeddings. If an unspsc_code is provided, it adds a where clause to filter the results by that code as well.\nOutput: Returns a list of the most relevant SP criteria documents and metadata found.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 11: Implement SP Criteria Suggestion Tool (REVISED - Added Local Import)\n# ==============================================================================\nprint(\"\\n--- Defining Step 11 Function: suggest_sp_criteria (Revised - Added Local Import) ---\")\n\n# --- Add specific import here for robustness ---\nfrom typing import Optional\n\n# --- Check Prerequisites ---\n# (Prerequisite checks remain the same)\nprerequisites_met_s11 = True\nif 'chroma_client' not in locals() or chroma_client is None: print(\"Error (Step 11): ChromaDB client not initialized.\"); prerequisites_met_s11 = False\nif 'genai_client_initialized' not in locals() or not genai_client_initialized: print(\"Error (Step 11): GenAI client not initialized.\"); prerequisites_met_s11 = False\nif 'gemini_ef' not in locals() or gemini_ef is None: print(\"Error (Step 11): Embedding function 'gemini_ef' not defined.\"); prerequisites_met_s11 = False\nif 'EMBEDDING_MODEL_NAME' not in locals(): EMBEDDING_MODEL_NAME = \"models/text-embedding-004\"; print(f\"Warning (Step 11): EMBEDDING_MODEL_NAME defaulting.\")\nSP_CRITERIA_COLLECTION_NAME = \"sp_criteria\"; sp_criteria_collection = None\nif prerequisites_met_s11:\n    if 'sp_criteria_collection' in locals() and sp_criteria_collection is not None and hasattr(sp_criteria_collection, 'name') and sp_criteria_collection.name == SP_CRITERIA_COLLECTION_NAME: print(\"Using existing 'sp_criteria_collection' object.\")\n    else:\n        print(f\"Attempting to retrieve collection '{SP_CRITERIA_COLLECTION_NAME}'...\");\n        try:\n            if 'chroma_client' in locals() and chroma_client: sp_criteria_collection = chroma_client.get_collection(name=SP_CRITERIA_COLLECTION_NAME); print(f\"Retrieved collection '{SP_CRITERIA_COLLECTION_NAME}'.\")\n            else: raise ValueError(\"ChromaDB client not available.\")\n        except Exception as e: print(f\"Error retrieving collection '{SP_CRITERIA_COLLECTION_NAME}': {e}\"); prerequisites_met_s11 = False\nelse: print(\"Cannot define Step 11 function due to prerequisites.\")\n\n\n# --- SP Criteria Suggestion Function (Revised - Explicit Embedding) ---\ndef suggest_sp_criteria(item_description: str, unspsc_code: Optional[str] = None, top_n: int = 5) -> list:\n    \"\"\"\n    Suggests relevant SP criteria by querying the sp_criteria collection\n    based on the semantic similarity of the item description, optionally filtered by UNSPSC code.\n    Explicitly generates query embedding.\n    \"\"\"\n    global sp_criteria_collection, gemini_ef, EMBEDDING_MODEL_NAME\n    local_prereqs_ok = True\n    if 'sp_criteria_collection' not in globals() or sp_criteria_collection is None: print(\"Error: SP Criteria Collection object missing.\"); local_prereqs_ok = False\n    if 'gemini_ef' not in globals() or gemini_ef is None: print(\"Error: Embedding function 'gemini_ef' not available.\"); local_prereqs_ok = False\n    if 'genai_client_initialized' not in globals() or not genai_client_initialized: print(\"Error: GenAI client not initialized.\"); local_prereqs_ok = False\n    if 'EMBEDDING_MODEL_NAME' not in globals(): print(\"Error: EMBEDDING_MODEL_NAME not defined.\"); local_prereqs_ok = False\n    if not local_prereqs_ok: return [{\"error\":\"Prerequisites missing inside suggest_sp_criteria.\"}]\n    if not item_description or not item_description.strip(): return [{\"error\":\"Item description empty\"}]\n\n    print(f\"\\n--- Suggesting SP criteria for: '{item_description}' (Code Filter: {unspsc_code}) ---\")\n    try:\n        # 1. Explicitly Generate Query Embedding\n        print(f\"Generating query embedding...\"); query_response = genai.embed_content(model=EMBEDDING_MODEL_NAME, content=item_description, task_type=\"RETRIEVAL_QUERY\"); query_embedding = query_response['embedding']\n        if not query_embedding or len(query_embedding) != 768: raise ValueError(f\"Generated query embedding has unexpected dimension: {len(query_embedding) if query_embedding else 'None'}.\")\n        print(f\"Query embedding generated.\");\n        # 2. Build Filter\n        where_filter = {'unspsc': unspsc_code.strip()} if unspsc_code and isinstance(unspsc_code, str) and unspsc_code.strip() else None\n        if where_filter: print(f\"Applying filter: {where_filter}\")\n        # 3. Query the collection\n        results = sp_criteria_collection.query(query_embeddings=[query_embedding], n_results=top_n, where=where_filter, include=['metadatas', 'documents', 'distances'])\n        result_count = len(results.get('ids', [])[0]) if results.get('ids') else 0; print(f\"Query returned {result_count} results.\")\n        # 4. Process results\n        suggestions = []\n        if results and results.get('ids') and results['ids'][0]:\n            ids, dists, metas, docs = results['ids'][0], results['distances'][0], results['metadatas'][0], results['documents'][0]\n            for i in range(len(ids)): suggestions.append({'id': ids[i], 'distance': dists[i], 'metadata': metas[i], 'document': docs[i]})\n        return suggestions\n    except Exception as e: print(f\"An error occurred during SP criteria suggestion query: {e}\"); traceback.print_exc(); return [{\"error\": f\"Query failed: {e}\"}]\n\nif 'suggest_sp_criteria' in globals(): print(\"Step 11 function 'suggest_sp_criteria' (re)defined with explicit embedding and local import.\")\n# ==============================================================================\n# (End of Step 11 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.114856Z","iopub.execute_input":"2025-04-27T09:43:35.115119Z","iopub.status.idle":"2025-04-27T09:43:35.141827Z","shell.execute_reply.started":"2025-04-27T09:43:35.115096Z","shell.execute_reply":"2025-04-27T09:43:35.140731Z"}},"outputs":[{"name":"stdout","text":"\n--- Defining Step 11 Function: suggest_sp_criteria (Revised - Added Local Import) ---\nAttempting to retrieve collection 'sp_criteria'...\nRetrieved collection 'sp_criteria'.\nStep 11 function 'suggest_sp_criteria' (re)defined with explicit embedding and local import.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"Step 12: Implement Evaluation Module\nThis step defines the eval_pairwise_sp_guidance function for comparing RAG vs. Search-augmented responses.\nInput: Takes project context, Response A (RAG), and Response B (Search-augmented).\nEvaluation Prompt: Uses the detailed QA_PAIRWISE_PROMPT to instruct an evaluator LLM.\nReasoning Generation: Calls the LLM to get a detailed comparison.\nStructured Choice Generation: Calls the LLM again using JSON mode to extract a structured 'A'/'B'/'SAME' choice based on the reasoning.\nOutput: Returns both the reasoning text and the structured choice, ensuring the reasoning clearly attributes findings to Response A and Response B for traceability.\n","metadata":{}},{"cell_type":"code","source":"import os\nimport google.generativeai as genai # Assuming 'genai' client is configured\n# Import types for Tool definition - adjust if your library version uses a different path\ntry:\n    # Newer versions might use google.ai.generativelanguage\n    import google.ai.generativelanguage as glm\n    print(\"Imported google.ai.generativelanguage as glm\")\nexcept ImportError:\n    # Older versions might use google.generativeai.types\n    try:\n         import google.generativeai.types as glm\n         print(\"Imported google.generativeai.types as glm\")\n    except ImportError:\n         print(\"Warning: Could not import types for Tool definition. Structured output might fail.\")\n         glm = None\n\nimport pandas as pd # Keep import if needed\nimport numpy as np\nimport pickle\nimport time\nimport json # For printing example output\nimport enum # For the AnswerComparison Enum\nimport functools # For caching if desired - though not strictly needed here\n\n# --- Check Prerequisites from previous steps ---\nprerequisites_met = True\n# Need genai client from Step 3\nif 'genai_client_initialized' not in locals() or not genai_client_initialized:\n    print(\"Error: Google Generative AI Client not initialized. Please ensure Step 3 ran successfully.\")\n    prerequisites_met = False\nif glm is None:\n     print(\"Error: Google AI types (glm) could not be imported. Structured output cannot be configured.\")\n     prerequisites_met = False\n\n# --- Evaluation Prompt (as provided by user - Completed) ---\nQA_PAIRWISE_PROMPT = \"\"\"\\\n# Instruction\nYou are an expert evaluator. Your task is to evaluate the quality of the responses generated by two AI models regarding sustainable procurement advice, given some context or a user prompt. We will provide you with the context/prompt and a pair of AI-generated responses (Response A and Response B). You should first read the inputs carefully for analyzing the task, and then evaluate the quality of the responses based on the Criteria provided in the Evaluation section below.\n\nYou will first judge responses individually, following the Rating Rubric and Evaluation Steps. Then you will give step-by-step explanations for your judgment, clearly attributing comments to Response A or Response B. Finally, you will compare results to declare the winner based on the Rating Rubric and Evaluation Steps.\n\n# Evaluation\n## Metric Definition\nYou will be assessing the quality of sustainable procurement guidance, which measures the overall quality of the advice/answer relative to the user prompt or context. Pay special attention to criteria like actionability, relevance, groundedness (to internal knowledge bases or external searches if applicable), and clarity.\n\n## Criteria\nInstruction following: The response demonstrates a clear understanding of the implied task (providing relevant SP guidance) or explicit instructions in the prompt.\nGroundedness: Response A should primarily reflect information from internal knowledge bases (like SP criteria, risk databases). Response B might incorporate external search findings. Evaluate if each response stays within its expected scope and if external information in B is relevant and properly integrated.\nCompleteness & Relevance: The response provides reasonably complete advice for the given context, focusing on relevant SP aspects.\nActionability & Clarity: The advice is clear, easy to understand, and suggests concrete actions or considerations for the project team.\nTraceability: The evaluation explanation must clearly link assessments back to specific aspects of Response A or Response B.\n\n## Rating Rubric\n\"A\": Response A provides better sustainable procurement guidance as per the criteria compared to Response B.\n\"SAME\": Response A and B provide equally good sustainable procurement guidance as per the criteria.\n\"B\": Response B provides better sustainable procurement guidance as per the criteria compared to Response A.\n\n## Evaluation Steps\nSTEP 1: Analyze Response A based on the quality criteria: Determine how well Response A fulfills the user requirements, is grounded appropriately, is complete, relevant, actionable, and clear.\nSTEP 2: Analyze Response B based on the quality criteria: Determine how well Response B fulfills the user requirements, is grounded appropriately (considering potential web search grounding), is complete, relevant, actionable, and clear.\nSTEP 3: Compare the overall performance of Response A and Response B based on your analyses and assessment, explicitly referencing points from each response.\nSTEP 4: Output your preference of \"A\", \"SAME\" or \"B\" based *only* on the Rating Rubric and your comparison. This should be the very last part of your reasoning if included here, or provided separately when asked for the structured output.\nSTEP 5: Provide your step-by-step assessment reasoning as requested, ensuring traceability to A and B.\n\n# User Inputs and AI-generated Responses\n## User Context/Prompt\n{prompt_context}\n\n# AI-generated Response\n\n### Response A\n{response_a}\n\n### Response B\n{response_b}\n\n# Evaluation Output\n\"\"\" # <-- Added closing tag for prompt\n\n\n# --- Define Enum for Structured Output ---\nclass AnswerComparison(enum.Enum):\n  A = 'A'\n  SAME = 'SAME'\n  B = 'B'\n\n\n# --- Evaluation Function ---\n# @functools.cache # Optional: Cache results if the same comparison is likely to be made multiple times\ndef eval_pairwise_sp_guidance(prompt_context: str, response_a: str, response_b: str) -> tuple[str | None, AnswerComparison | None]:\n    \"\"\"\n    Evaluates two SP guidance responses using Gemini, returning detailed reasoning and a structured choice.\n\n    Args:\n        prompt_context: The context or user prompt that led to the responses.\n        response_a: The first response (e.g., RAG-based).\n        response_b: The second response (e.g., Search-augmented).\n\n    Returns:\n        A tuple containing:\n        - str | None: The detailed textual reasoning from the evaluator model, or None on error.\n        - AnswerComparison | None: The structured comparison result (A, B, or SAME), or None on error.\n    \"\"\"\n    if not prerequisites_met:\n        print(\"Error: Prerequisites (GenAI client, types import) not met. Cannot perform evaluation.\")\n        return None, None\n    if not all([prompt_context, response_a, response_b]):\n         print(\"Error: Prompt context and both responses are required for evaluation.\")\n         return None, None\n\n    print(f\"\\n--- Evaluating SP Guidance Responses ---\")\n    reasoning_text = None\n    structured_choice = None\n    try:\n        # Use a chat model for multi-turn interaction (reasoning first, then structured output)\n        # Choose a capable model, e.g., gemini-1.5-flash or gemini-pro\n        eval_model_name = 'gemini-2.0-flash-latest'\n        print(f\"Using evaluation model: {eval_model_name}\")\n        # Create a *new* chat session for each evaluation to keep contexts separate\n        eval_chat = genai.GenerativeModel(eval_model_name).start_chat()\n\n        # 1. Get Detailed Reasoning\n        print(\"Generating detailed evaluation reasoning...\")\n        reasoning_prompt = QA_PAIRWISE_PROMPT.format(\n            prompt_context=prompt_context,\n            response_a=response_a,\n            response_b=response_b\n        )\n        reasoning_response = eval_chat.send_message(reasoning_prompt)\n        # Check if response has text (handle potential blocks/errors)\n        if reasoning_response and hasattr(reasoning_response, 'text'):\n            reasoning_text = reasoning_response.text\n            print(\"Reasoning generated.\")\n            # print(\"Reasoning Text (preview):\\n\", reasoning_text[:500] + \"...\") # Optional print\n        else:\n            print(\"Warning: No text found in reasoning response.\")\n            print(f\"Full reasoning response object: {reasoning_response}\")\n            reasoning_text = f\"Error: No text generated. Response: {reasoning_response}\"\n\n\n        # 2. Get Structured Choice using JSON mode\n        print(\"\\nGenerating structured choice (A/B/SAME)...\")\n        if glm: # Check if types import succeeded\n             structured_output_config = glm.GenerationConfig(\n                 response_mime_type=\"application/json\",\n                 response_schema=AnswerComparison, # Pass the Enum class directly\n                 temperature=0.0 # Low temp for consistent choice extraction\n             )\n             # Ask the model to extract the choice from its previous reasoning\n             choice_response = eval_chat.send_message(\n                 \"Based on your previous detailed evaluation, what is the final rating ('A', 'B', or 'SAME') according to the Rating Rubric?\",\n                 generation_config=structured_output_config\n             )\n\n             # Parse the structured response\n             # Accessing parsed JSON might vary slightly by library version\n             if choice_response and hasattr(choice_response, 'text'): # Check if response has text/parts\n                 try:\n                     # In newer versions, response.text might directly contain the JSON string\n                     # Or access via parts if needed: json_string = choice_response.parts[0].text\n                     parsed_json = json.loads(choice_response.text) # Assuming text holds the JSON\n                     # Validate against Enum\n                     structured_choice = AnswerComparison(parsed_json['value']) # Assuming schema puts result in 'value' key\n                     print(f\"Structured choice extracted: {structured_choice}\")\n                 except (json.JSONDecodeError, KeyError, ValueError, AttributeError) as e:\n                     print(f\"Error parsing structured choice from response: {e}\")\n                     print(f\"Raw choice response text: {choice_response.text if hasattr(choice_response, 'text') else 'N/A'}\")\n                     # Fallback: Try simple text extraction if JSON fails\n                     if reasoning_text and \"Output your preference\" in reasoning_text:\n                          # Very basic attempt to find A/B/SAME at the end of reasoning\n                          last_part = reasoning_text.rsplit(maxsplit=1)[-1].strip().replace('\"','').replace('.','')\n                          try:\n                               structured_choice = AnswerComparison(last_part)\n                               print(f\"Structured choice extracted via fallback: {structured_choice}\")\n                          except ValueError:\n                               print(\"Fallback failed.\")\n             else:\n                 print(\"Warning: No text/parts found in structured choice response.\")\n                 print(f\"Full choice response object: {choice_response}\")\n\n        else:\n             print(\"Skipping structured choice generation because type import (glm) failed earlier.\")\n\n\n        return reasoning_text, structured_choice\n\n    except Exception as e:\n        print(f\"An error occurred during pairwise evaluation: {e}\")\n        # import traceback\n        # traceback.print_exc()\n        return reasoning_text, None # Return any reasoning generated before the error\n\n# --- Example Usage (Optional - Uncomment to test) ---\nif prerequisites_met:\n    print(\"\\n--- Running Example Pairwise Evaluation ---\")\n    # Create dummy context and responses for testing\n    dummy_context = \"Project involves procuring 100 laptops for deployment in Ghana.\"\n    dummy_response_a = \"Based on SP Criteria DB: Ensure laptops meet EPEAT Silver standard (UNSPSC 43211503). Check clauses for waste disposal. See SDG 12.\"\n    dummy_response_b = \"Web search indicates Ghana has new e-waste regulations (Ref: URL). Suggest EPEAT Gold standard due to context. Also confirm supplier compliance with local labor laws mentioned in partner X's policy (Ref: URL). Addresses SDG 12, 8.\"\n\n    # print(f\"Context: {dummy_context}\")\n    # print(f\"Response A: {dummy_response_a}\")\n    # print(f\"Response B: {dummy_response_b}\")\n\n    # eval_reasoning, eval_choice = eval_pairwise_sp_guidance(dummy_context, dummy_response_a, dummy_response_b)\n\n    # print(\"\\n--- Evaluation Results ---\")\n    # print(\"Reasoning:\\n\", eval_reasoning)\n    # print(\"\\nStructured Choice:\", eval_choice) # Will be <AnswerComparison.A: 'A'> or similar\n\n    print(\"\\n--- Example Usage Complete (results printed above if uncommented) ---\")\nelse:\n     print(\"\\nSkipping example usage due to missing prerequisites.\")\n\n\n# --- Final Status ---\nprint(\"\\n--- Step 12 Final Status ---\")\nif prerequisites_met and 'eval_pairwise_sp_guidance' in locals():\n    print(\"Step 12 Completed: Function 'eval_pairwise_sp_guidance' is defined.\")\n    print(\"You can now call this function with context, response_a, and response_b.\")\nelse:\n    print(\"Step 12 Incomplete: Failed to define function or prerequisites missing.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.143175Z","iopub.execute_input":"2025-04-27T09:43:35.143498Z","iopub.status.idle":"2025-04-27T09:43:35.167613Z","shell.execute_reply.started":"2025-04-27T09:43:35.143473Z","shell.execute_reply":"2025-04-27T09:43:35.166621Z"}},"outputs":[{"name":"stdout","text":"Imported google.ai.generativelanguage as glm\n\n--- Running Example Pairwise Evaluation ---\n\n--- Example Usage Complete (results printed above if uncommented) ---\n\n--- Step 12 Final Status ---\nStep 12 Completed: Function 'eval_pairwise_sp_guidance' is defined.\nYou can now call this function with context, response_a, and response_b.\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"Step 13: Implement Final Recommendation Generation\nThis step defines the generate_final_recommendation function to synthesize all gathered information.\nInput: Takes all outputs from previous tools (context, UNSPSC info, risks, criteria, search results, evaluation results).\nSynthesize with LLM: Constructs a comprehensive prompt containing all inputs.\nPrompt Engineering: Instructs a capable LLM (e.g., gemini-1.5-flash-latest) to synthesize the information, prioritize based on the evaluation, ensure traceability, and generate a final recommendation formatted in Markdown with clear sections (Key Considerations, Recommended Actions/Criteria, Further Investigation).\nOutput: Returns the final recommendation text as a Markdown string.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 13 Code Block - Final Recommendation Generation Function (Revised for Markdown)\n# ==============================================================================\nimport os\nimport google.generativeai as genai\ntry: import google.ai.generativelanguage as glm\nexcept ImportError:\n    try: import google.generativeai.types as glm\n    except ImportError: glm = None\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport time\nimport json\nimport enum\nfrom typing import Optional, List, Dict\n\n# --- Ensure Enum from Step 12 is available ---\nif 'AnswerComparison' not in locals():\n     print(\"Warning: AnswerComparison Enum not defined. Redefining.\")\n     class AnswerComparison(enum.Enum): A = 'A'; SAME = 'SAME'; B = 'B'\n\n# --- Check Prerequisites ---\nprerequisites_met_s13 = True\nif 'genai_client_initialized' not in locals() or not genai_client_initialized:\n    print(\"Error (Step 13): GenAI client not initialized.\"); prerequisites_met_s13 = False\n\n# --- Final Recommendation Function (Revised Prompt for Markdown) ---\ndef generate_final_recommendation(\n    project_context: dict,\n    unspsc_info: list | None,\n    risk_info: dict | None,\n    suggested_criteria: list | None,\n    search_results: str | None,\n    eval_reasoning: str | None,\n    eval_choice: AnswerComparison | None\n    ) -> str | None:\n    \"\"\"\n    Generates the final sustainable procurement recommendation in Markdown format.\n    (Args and Returns documentation updated)\n    \"\"\"\n    if not prerequisites_met_s13:\n        print(\"Error: Prerequisites (GenAI client) not met. Cannot generate recommendation.\")\n        return None\n\n    print(\"\\n--- Generating Final Recommendation (Markdown Format) ---\")\n\n    # (Helper function format_for_prompt remains the same)\n    def format_for_prompt(data, max_items=5, max_len=1000):\n        # ... (implementation from previous version) ...\n        if data is None: return \"Not available or not generated.\"\n        if isinstance(data, str):\n             if not data.strip(): return \"Not available or empty.\"\n             return data[:max_len] + ('...' if len(data) > max_len else '')\n        if isinstance(data, list):\n            if not data: return \"None found or provided.\"\n            items_to_show = data[:max_items]\n            formatted_items = json.dumps(items_to_show, indent=2, default=str)\n            suffix = f\"\\n... (showing first {max_items} of {len(data)})\" if len(data) > max_items else \"\"\n            return formatted_items + suffix\n        if isinstance(data, dict):\n             if 'family_risks' in data or 'country_risks' in data or 'segment_risks' in data: # Added segment_risks check\n                  seg_risks = data.get('segment_risks', {}) # Check segment risks too\n                  cou_risks = data.get('country_risks', {})\n                  seg_meta = seg_risks.get('metadatas', [])\n                  cou_meta = cou_risks.get('metadatas', [])\n                  seg_meta_str = json.dumps(seg_meta[:max_items], indent=2) if seg_meta else \"[]\"\n                  cou_meta_str = json.dumps(cou_meta[:max_items], indent=2) if cou_meta else \"[]\"\n                  return (f\"Segment Risks Found: {seg_risks.get('count', 0)}\\n\" # Changed from Family\n                          f\"Segment Metadatas (sample): {seg_meta_str}\\n\"\n                          f\"Country Risks Found: {cou_risks.get('count', 0)}\\n\"\n                          f\"Country Metadatas (sample): {cou_meta_str}\")\n             else:\n                  try: return json.dumps(data, indent=2, default=str)[:max_len] + ('...' if len(json.dumps(data)) > max_len else '')\n                  except TypeError: return str(data)[:max_len] + ('...' if len(str(data)) > max_len else '')\n        return str(data)\n\n    # Pre-format sections\n    formatted_context = format_for_prompt(project_context)\n    formatted_unspsc = format_for_prompt(unspsc_info, max_items=1)\n    formatted_risks = format_for_prompt(risk_info) # Will now show segment risks\n    formatted_criteria = format_for_prompt(suggested_criteria, max_items=5)\n    formatted_search = format_for_prompt(search_results)\n    formatted_eval_choice = eval_choice.name if eval_choice else \"Not Available\"\n    formatted_eval_reasoning = format_for_prompt(eval_reasoning)\n\n    # --- UPDATED Synthesis Prompt Requesting Markdown ---\n    prompt = f\"\"\"\nYou are an expert assistant specializing in Sustainable Procurement (SP).\nYour task is to synthesize information gathered about a specific procurement project and provide clear, actionable recommendations to the project team, formatted as a Markdown report section.\n\n**1. Project Context:**\n{formatted_context}\n\n**2. Identified Procurement Category (UNSPSC):**\n{formatted_unspsc}\n\n**3. Identified Potential Risks:**\n{formatted_risks}\n\n**4. Suggested SP Criteria (from Internal Database - Top 5):**\n{formatted_criteria}\n(This represents Response A in the evaluation context)\n\n**5. Contextual Research Findings (from Web Search):**\n{formatted_search}\n(This represents information potentially used in Response B in the evaluation context)\n\n**6. Evaluation of Internal vs. External Info:**\nAn evaluation compared purely internal suggestions (Response A) against suggestions potentially augmented by web search (Response B).\n* **Evaluation Choice:** {formatted_eval_choice}\n* **Evaluation Reasoning Summary:**\n    {formatted_eval_reasoning}\n\n**Your Task:**\n\nBased *only* on the information provided above, generate a concise, clear, and actionable Sustainable Procurement recommendation **formatted using Markdown**. Use headings (`##`), bullet points (`*` or `-`), and bold text (`**text**`) appropriately to structure the report section.\n\n* **## Key Sustainability Considerations:** Briefly summarize the most salient risks (Segment and Country) and opportunities identified from all sources.\n* **## Recommended Actions/Criteria:**\n    * Clearly list the specific SP criteria (e.g., criteria text, documentation needs from Input Section 4) that are most relevant, particularly if Evaluation Choice was 'A' or 'SAME'. Use bullet points.\n    * Complement these internal criteria with actionable insights or context derived from the Contextual Research Findings / Response B (Input Section 5), especially if Evaluation Choice was 'B' or 'SAME'. Synthesize these logically. Clearly state how the external findings modify or add to the internal criteria.\n    * Justify key recommendations by briefly referencing the source (e.g., \"Due to the flagged country risk of X...\", \"Considering the web search finding about regulation Y [Source: Web Search]...\", \"Internal criteria database suggests Z...\"). Ensure it's clear what comes from internal data (A) and what comes from external research (B). Use bold text for emphasis on key actions.\n* **## Further Investigation (Optional):** Suggest any specific areas needing further investigation by the project team, using bullet points.\n\nBe practical and focus on actionable advice. Ensure clear traceability in your recommendations back to the provided inputs. Output **only** the Markdown formatted report section.\n\"\"\"\n    # --- End UPDATED Prompt ---\n\n    # --- Call Gemini Model ---\n    try:\n        synthesis_model_name = 'gemini-1.5-flash-latest'\n        print(f\"Using synthesis model: {synthesis_model_name}\")\n        model = genai.GenerativeModel(synthesis_model_name)\n        print(\"Sending synthesis prompt to Gemini...\")\n        request_options = {\"timeout\": 300}\n        response = model.generate_content(\n             prompt, request_options=request_options\n        )\n        print(\"Final recommendation generated by model.\")\n        # (Response handling logic remains the same)\n        if response and hasattr(response, 'text'):\n             if 'i cannot provide' in response.text.lower() or 'i am unable to' in response.text.lower(): print(\"Warning: Model may have refused.\")\n             return response.text\n        elif response and hasattr(response, 'prompt_feedback') and response.prompt_feedback.block_reason:\n             print(f\"Warning: Prompt was blocked. Reason: {response.prompt_feedback.block_reason}\")\n             return f\"Error: Recommendation generation blocked. Reason: {response.prompt_feedback.block_reason}\"\n        else: print(\"Warning: Response has no text content.\"); return \"Error: Failed to generate recommendation text.\"\n    except Exception as e: print(f\"An error occurred: {e}\"); return f\"Error: {e}\"\n\nif 'generate_final_recommendation' in globals():\n     print(\"Step 13 function 'generate_final_recommendation' (re)defined with Markdown output request.\")\n# ==============================================================================\n# (End of Step 13 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.169399Z","iopub.execute_input":"2025-04-27T09:43:35.169724Z","iopub.status.idle":"2025-04-27T09:43:35.195158Z","shell.execute_reply.started":"2025-04-27T09:43:35.169695Z","shell.execute_reply":"2025-04-27T09:43:35.193965Z"}},"outputs":[{"name":"stdout","text":"Step 13 function 'generate_final_recommendation' (re)defined with Markdown output request.\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"Step 14: Implement Agent Framework (LangGraph) (Revised)\nThis final step integrates all tools into a stateful agent using LangGraph.\nDefine Agent State: Defines a structure (AgentState) to hold conversation messages.\nDefine Tools: Wraps the Python functions from Steps 8, 9 (Revised), 10, and 11 (Revised) using the @tool decorator.\nConfigure Agent LLM: Initializes ChatGoogleGenerativeAI (gemini-1.5-flash-latest), binds the tools, and sets safety settings.\nDefine Graph Nodes: Creates agent_node (calls LLM with system prompt and history to decide action) and tool_node (executes chosen tool).\nDefine Graph Edges: Uses conditional logic (should_continue) to route flow between the agent node and the tool node, or to end the turn. The agent loops back after a tool call to process the result.\nCompile Graph: Compiles the nodes and edges into a runnable agent_graph.\nInteraction Loop: Defines a function (run_agent_interaction) containing a while loop to handle user input, invoke the agent_graph.stream method (showing ReAct-style steps), print outputs, and manage the conversation history (messages).\nThis framework allows the agent to manage multi-turn interactions, follow the defined workflow, use tools autonomously, and provide reasoned sustainable procurement advice.\n","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# Step 14 Code Block - LangGraph Agent Framework (Final)\n# ==============================================================================\n# Assumes Steps 2-13 defining necessary variables and functions\n# have been executed successfully in the current kernel session.\n\nimport os\nimport google.generativeai as genai\ntry: import google.ai.generativelanguage as glm # Newer google.ai namespace\nexcept ImportError:\n    try: import google.generativeai.types as glm # Older google.generativeai.types namespace\n    except ImportError: glm = None; print(\"Warning: GLM types for HarmCategory not found.\")\n\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport time\nimport json\nimport enum\nimport chromadb\nfrom chromadb.utils import embedding_functions\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport operator\nfrom typing import TypedDict, Annotated, Sequence, List, Dict, Any, Optional\n\n# --- LangChain / LangGraph Imports ---\nfrom langchain_core.messages import BaseMessage, FunctionMessage, HumanMessage, AIMessage, SystemMessage\nfrom langchain.tools import tool\nfrom langchain_google_genai import ChatGoogleGenerativeAI\nfrom langgraph.graph import StateGraph, END\n# --- Corrected Import for ToolExecutor & ToolInvocation ---\n# Using standard path, assuming pinned langgraph version makes it work\ntry:\n    from langgraph.prebuilt import ToolExecutor, ToolInvocation\n    print(\"Imported ToolExecutor and ToolInvocation from langgraph.prebuilt\")\nexcept ImportError as e:\n     print(f\"ERROR: Failed to import ToolExecutor/ToolInvocation from langgraph.prebuilt: {e}\")\n     print(\"Check installed langgraph version (~=0.1.0) and its structure.\")\n     ToolExecutor = None\n     ToolInvocation = None\n# --- End Corrected Import ---\nfrom langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n\n\nprint(\"\\n--- Loading Prerequisites for Agent Framework ---\")\n\n# --- Ensure Enum from Step 12 is available ---\nif 'AnswerComparison' not in locals():\n     print(\"Warning: AnswerComparison Enum not defined. Redefining.\")\n     class AnswerComparison(enum.Enum): A = 'A'; SAME = 'SAME'; B = 'B'\n\n# --- Ensure prerequisite functions/data/clients are loaded/defined ---\n# Includes checks and attempts to re-initialize/reload if possible\nprerequisites_ok = True\n# Define embedding function first (needed for collection retrieval)\ngemini_ef = None\nif 'gemini_ef' not in locals() or gemini_ef is None:\n     print(\"Attempting to define/redefine embedding function 'gemini_ef'...\")\n     try:\n        api_key = os.environ.get(\"GOOGLE_API_KEY\");\n        if not api_key: raise ValueError(\"GOOGLE_API_KEY needed.\")\n        EMBEDDING_MODEL_NAME=\"models/text-embedding-004\"\n        gemini_ef = embedding_functions.GoogleGenerativeAiEmbeddingFunction(api_key=api_key, model_name=EMBEDDING_MODEL_NAME)\n        print(\"Defined Gemini embedding function.\")\n     except Exception as e: print(f\"Error defining embedding function: {e}\"); prerequisites_ok = False\n\n# Check clients\nif 'chroma_client' not in locals() or chroma_client is None:\n     print(\"Attempting to re-initialize ChromaDB client...\")\n     try:\n          chroma_persist_directory = \"./chroma_db\"; os.makedirs(chroma_persist_directory, exist_ok=True)\n          chroma_client = chromadb.PersistentClient(path=chroma_persist_directory); chroma_client.heartbeat()\n          print(\"ChromaDB client re-initialized.\")\n     except Exception as e: print(f\"ERROR: Failed to re-initialize ChromaDB client: {e}\"); prerequisites_ok = False\nif 'genai_client_initialized' not in locals() or not genai_client_initialized:\n     print(\"Attempting to re-initialize GenAI client...\")\n     try:\n          api_key = os.environ.get(\"GOOGLE_API_KEY\");\n          if not api_key: raise ValueError(\"GOOGLE_API_KEY needed.\")\n          genai.configure(api_key=api_key)\n          # Add retry logic here if needed (from Step 3 corrected code)\n          genai_client_initialized = True; print(\"GenAI client re-initialized.\")\n     except Exception as e: print(f\"ERROR: Failed to re-initialize GenAI client: {e}\"); prerequisites_ok = False\n# Check data\nif 'precomputed_unspsc_data' not in locals() or precomputed_unspsc_data is None:\n     precomputed_data_path = \"unspsc_embeddings_data_full.pkl\"\n     if os.path.exists(precomputed_data_path):\n          print(f\"Reloading {precomputed_data_path}...\")\n          try:\n               with open(precomputed_data_path, 'rb') as f: precomputed_unspsc_data = pickle.load(f)\n               if not (isinstance(precomputed_unspsc_data, list) and len(precomputed_unspsc_data) > 0): raise TypeError(\"Invalid format\")\n               print(\"Reloaded UNSPSC data.\")\n               # Verification print for segment code format\n               if len(precomputed_unspsc_data) > 0 and isinstance(precomputed_unspsc_data[0], dict):\n                    example_seg_code = precomputed_unspsc_data[0].get('segment_code', 'MISSING')\n                    print(f\"Verification: Example segment_code from loaded data: '{example_seg_code}' (Type: {type(example_seg_code)})\")\n                    if not isinstance(example_seg_code, str) or len(example_seg_code) != 8 or not example_seg_code.isdigit():\n                         print(\"WARNING: Example segment_code does not appear to be an 8-digit string. Check Step 5 data preparation.\")\n          except Exception as e: print(f\"ERROR: Failed to reload/verify {precomputed_data_path}: {e}\"); prerequisites_ok = False\n     else: print(f\"ERROR: {precomputed_data_path} not found.\"); prerequisites_ok = False\n# Check collection objects (using embedding function)\ncollection_objects = {}\ncollection_names_s14 = {\n    \"sp_criteria_collection\": \"sp_criteria\",\n    \"segment_risk_collection\": \"unspsc_segment_risk\", # Correct name\n    \"country_risk_collection\": \"country_risk\"\n}\n# Assign retrieved collections to global scope if check succeeds\nif 'chroma_client' in locals() and chroma_client and prerequisites_ok and 'gemini_ef' in locals() and gemini_ef:\n     for var_name, coll_name in collection_names_s14.items():\n          if var_name not in globals() or globals().get(var_name) is None: # Check if global var exists\n               print(f\"Attempting to retrieve collection '{coll_name}'...\")\n               try:\n                    # Retrieve collection WITHOUT explicitly passing embedding function here\n                    coll = chroma_client.get_collection(name=coll_name)\n                    globals()[var_name] = coll; # Assign to global scope\n                    print(f\"Retrieved collection '{coll_name}'. Count: {coll.count()}\")\n               except Exception as e:\n                    print(f\"ERROR: Failed to retrieve collection '{coll_name}': {e}\")\n                    prerequisites_ok = False\n                    globals()[var_name] = None # Ensure it's None on failure\n          else:\n               print(f\"Collection object '{var_name}' already exists in global scope.\")\n\nelif not ('chroma_client' in locals() and chroma_client):\n     print(\"ERROR: Chroma client not available for collection checks.\")\n     prerequisites_ok = False\nelif not ('gemini_ef' in locals() and gemini_ef):\n     print(\"ERROR: Embedding function 'gemini_ef' not available.\")\n     prerequisites_ok = False\n\n# Check tool functions (ensure they are defined in the global scope)\n# Assuming functions like find_similar_unspsc, identify_risks etc. were defined in previous cells/steps\ntool_functions_to_check = ['find_similar_unspsc', 'identify_risks', 'research_context_with_grounding','suggest_sp_criteria']\nfor func_name in tool_functions_to_check:\n     if func_name not in globals(): print(f\"ERROR: Function '{func_name}' not defined.\"); prerequisites_ok = False\n# Check if ToolExecutor/ToolInvocation were imported successfully\nif ToolExecutor is None or ToolInvocation is None:\n     print(\"ERROR: ToolExecutor or ToolInvocation import failed. Cannot proceed.\")\n     prerequisites_ok = False\n\n\n# --- Agent Definition starts only if prerequisites are OK ---\nif not prerequisites_ok:\n     print(\"\\nERROR: Cannot build agent framework due to missing prerequisites.\")\n     agent_graph = None # Ensure agent_graph is None if setup fails\nelse:\n    print(\"\\nPrerequisites seem OK. Defining agent framework...\")\n\n    # --- Define Tools using LangChain decorator (Using REVISED identify_risks_tool with validation) ---\n    @tool\n    def find_similar_unspsc_tool(item_description: str, top_n: int = 1) -> list:\n        \"\"\"Finds the top_n most semantically similar UNSPSC entries for a given item description. Use this FIRST to identify the category code (Commodity), Segment code, and Family code for procured items.\"\"\"\n        # This tool needs access to the 'find_similar_unspsc' function and 'precomputed_unspsc_data'\n        print(f\"Executing find_similar_unspsc_tool for: '{item_description}'\")\n        if 'find_similar_unspsc' in globals() and 'precomputed_unspsc_data' in globals() and precomputed_unspsc_data:\n            try: return find_similar_unspsc(item_description, precomputed_unspsc_data, top_n)\n            except Exception as e: return [{\"error\": f\"Error in find_similar_unspsc: {e}\"}]\n        else: return [{\"error\": \"UNSPSC linking tool prerequisites not met.\"}]\n\n    @tool\n    def identify_risks_tool(segment_code: str, country_identifier: str, search_by_iso: bool = True) -> dict:\n        \"\"\"Identifies sustainability risks based on UNSPSC Segment code and country identifier (name or ISO code). Requires an 8-digit segment_code (from find_similar_unspsc_tool) and country_identifier. This tool performs a direct lookup in the risk database based on the segment code; it does not need semantic context about the segment.\"\"\"\n        print(f\"Executing identify_risks_tool for Segment: {segment_code}, Country: {country_identifier}\")\n        # Input Validation\n        if not isinstance(segment_code, str) or not segment_code.isdigit() or len(segment_code) != 8:\n             error_msg = f\"Invalid segment_code format received: '{segment_code}'. Expected an 8-digit string.\"\n             print(f\"ERROR in identify_risks_tool: {error_msg}\"); return {\"error\": error_msg}\n        # This tool needs access to the 'identify_risks' function\n        if 'identify_risks' in globals():\n            try: return identify_risks(segment_code=segment_code, country_identifier=country_identifier, search_by_iso=search_by_iso)\n            except Exception as e: return {\"error\": f\"Error in identify_risks: {e}\"}\n        else: return {\"error\": \"Risk identification function 'identify_risks' not defined.\"}\n\n    @tool\n    def research_context_tool(search_query: str) -> str | dict:\n        \"\"\"Performs a grounded web search using Google Search for contextual information. Requires a search_query string. Use this to find recent regulations, partner policies, or details when internal databases lack information.\"\"\"\n        # This tool needs access to the 'research_context_with_grounding' function\n        print(f\"Executing research_context_tool for query: '{search_query}'\")\n        if 'research_context_with_grounding' in globals():\n            try: result = research_context_with_grounding(search_query); return result if isinstance(result, (str, dict)) else str(result)\n            except Exception as e: return f\"Error in research_context_with_grounding: {e}\"\n        else: return \"Error: Contextual research tool function 'research_context_with_grounding' not defined.\"\n    @tool\n    def suggest_sp_criteria_tool(item_description: str, unspsc_code: Optional[str] = None, top_n: int = 5) -> list:\n        \"\"\"Suggests relevant SP criteria from the internal database based on the item description. Optionally filters by UNSPSC code if provided. Requires item_description string. Use AFTER find_similar_unspsc_tool to get the code.\"\"\"\n        # This tool needs access to the 'suggest_sp_criteria' function\n        print(f\"Executing suggest_sp_criteria_tool for: '{item_description}', Code: {unspsc_code}\")\n        if 'suggest_sp_criteria' in globals():\n             try: return suggest_sp_criteria(item_description=item_description, unspsc_code=unspsc_code, top_n=top_n)\n             except Exception as e: return [{\"error\": f\"Error in suggest_sp_criteria: {e}\"}]\n        else: return [{\"error\": \"SP criteria suggestion function 'suggest_sp_criteria' not defined.\"}]\n\n    # --- Define Agent State ---\n    class AgentState(TypedDict): messages: Annotated[Sequence[BaseMessage], operator.add]\n\n    # --- Define Tools and Tool Executor ---\n    available_tools = [ find_similar_unspsc_tool, identify_risks_tool, research_context_tool, suggest_sp_criteria_tool ]\n    tool_executor = ToolExecutor(available_tools)\n\n    # --- Define Agent LLM with Tools ---\n    llm_model_name = 'gemini-2.0-flash'; print(f\"Initializing agent LLM with model: {llm_model_name}\")\n    safety_settings_med = {};\n    if glm and hasattr(glm, 'HarmCategory'): safety_settings_med = { glm.HarmCategory.HARM_CATEGORY_HARASSMENT: 1, glm.HarmCategory.HARM_CATEGORY_HATE_SPEECH: 1, glm.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 1, glm.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 1 }\n    llm = ChatGoogleGenerativeAI(model=llm_model_name, convert_system_message_to_human=True, safety_settings=safety_settings_med if safety_settings_med else None, temperature=0.1)\n    llm_with_tools = llm.bind_tools(available_tools)\n\n    # --- Define Agent Logic Nodes ---\n    def agent_node(state: AgentState):\n        \"\"\"Invokes the agent model to determine the next step.\"\"\"\n        print(\"--- Agent Node ---\")\n        # System Prompt Using Segment Code for Risk and Validation\n        system_prompt = \"\"\"You are an expert assistant for Sustainable Procurement (SP). Your goal is to help users analyze their procurement projects for SP risks and opportunities. Follow these steps precisely:\n1.  Understand the Project: If context is missing (item description, country, quantity, partners), ask clarifying questions.\n2.  Identify Category: Once you have an item description, FIRST use the 'find_similar_unspsc_tool' to get the UNSPSC details. From the results, extract the 'code' (Commodity code) and the 'segment_code'. Ensure the 'segment_code' is the full 8-digit code (e.g., '43000000'). If multiple results, use the one with the highest similarity score. If no code is found, inform the user and ask for clarification or a different description.\n3.  Suggest Criteria: Use the 'suggest_sp_criteria_tool'. Provide BOTH the original 'item_description' AND the 'unspsc_code' (Commodity code) you extracted in the previous step as arguments.\n4.  Identify Risks: Use the 'identify_risks_tool' with the full 8-digit 'segment_code' (extracted in step 2) and the 'country_identifier' (ask user if missing). Verify you are passing the 8-digit segment code. If the tool returns an error about invalid segment code format, inform the user. If the tool returns no specific risks (count=0), state that clearly and proceed.\n5.  Research Context: After checking internal risks and criteria (Steps 3 & 4), **use the `research_context_tool`** to find relevant external context, especially if internal results were sparse or if the user's query or previous results mention specific regulations, standards, or partner policies needing verification. Formulate specific search queries based on the project context (e.g., \"Sustainability policies of [Partner Name]\", \"[Country Name] e-waste regulations 2024\", \"Child labor risks in [Sector/Segment] in [Country]\"). Report the key findings from the search.\n6.  Synthesize and Advise: Once you have gathered sufficient information (at least UNSPSC, risks, criteria, and context research), inform the user you will now generate the final recommendation. [NOTE: The final recommendation generation happens outside this tool-calling loop based on the gathered information in the message history, using the `generate_final_recommendation` function].\n\nThink step-by-step. Always call tools in the specified order when applicable (find UNSPSC code -> suggest criteria -> identify risks -> research context). Ask for clarification if needed. Provide final answers based *only* on the information gathered from the user and the tools.\"\"\"\n        prompt_messages = [SystemMessage(content=system_prompt)] + state['messages']\n        response = llm_with_tools.invoke(prompt_messages)\n        return {\"messages\": [response]}\n\n    def tool_node(state: AgentState):\n        \"\"\"Invokes the tool executor to call the chosen tool.\"\"\"\n        print(\"--- Action Node: Executing Tool ---\")\n        last_ai_message = state['messages'][-1]\n        if not isinstance(last_ai_message, AIMessage) or not last_ai_message.tool_calls: return {\"messages\": []}\n        tool_calls = last_ai_message.tool_calls; function_messages = []\n        for tool_call in tool_calls:\n            tool_name = tool_call['name']; print(f\"Invoking tool: {tool_name} with args: {tool_call['args']}\")\n            try:\n                action = ToolInvocation(tool=tool_name, tool_input=tool_call['args'])\n                tool_output = tool_executor.invoke(action)\n                print(f\"Tool '{tool_name}' output (preview): {str(tool_output)[:200]}...\")\n                function_messages.append(FunctionMessage(content=str(tool_output), name=tool_name))\n            except Exception as e: print(f\"Error executing tool {tool_name}: {e}\"); function_messages.append(FunctionMessage(content=f\"Error executing tool {tool_name}: {e}\", name=tool_name))\n        return {\"messages\": function_messages}\n\n    # --- Define Conditional Edge Logic ---\n    def should_continue(state: AgentState):\n        \"\"\"Determines whether to call tools or end.\"\"\"\n        last_message = state['messages'][-1]\n        if isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls: return \"call_tool\"\n        else: return \"end\"\n\n    # --- Build the Graph ---\n    print(\"\\n--- Building LangGraph Agent ---\")\n    graph_builder = StateGraph(AgentState); graph_builder.add_node(\"agent\", agent_node); graph_builder.add_node(\"action\", tool_node)\n    graph_builder.set_entry_point(\"agent\"); graph_builder.add_conditional_edges(\"agent\", should_continue, {\"call_tool\": \"action\", \"end\": END})\n    graph_builder.add_edge(\"action\", \"agent\")\n    agent_graph = graph_builder.compile(); print(\"Agent graph compiled.\")\n\n\n    # --- Helper to print messages (ReAct style) ---\n    def print_agent_step(step_output):\n         messages = step_output.get(\"messages\", [])\n         if not messages: return\n         message = messages[0]\n         if isinstance(message, HumanMessage): print(f\"\\nUSER: {message.content}\")\n         elif isinstance(message, AIMessage):\n              print(f\"\\nAI:\")\n              if message.content: print(f\"  {message.content}\")\n              if hasattr(message, 'tool_calls') and message.tool_calls:\n                   print(f\"  Tool Calls:\")\n                   for tc in message.tool_calls: print(f\"    - Tool: {tc['name']}, Args: {tc['args']}\")\n         elif isinstance(message, FunctionMessage): print(f\"\\nTOOL RESULT ({message.name}):\\n  {message.content}\")\n         else: print(f\"\\nUNKNOWN: {message}\")\n         print(\"---\")\n\n    # --- Function for Interaction Loop ---\n    def run_agent_interaction(graph, initial_user_message):\n        \"\"\"Runs the agent interaction loop.\"\"\"\n        print(\"\\n--- Starting Agent Interaction (Type 'quit' to exit) ---\")\n        messages = [HumanMessage(content=initial_user_message)]\n        while True:\n            print(\"\\nAgent is processing...\")\n            current_state = {\"messages\": messages}\n            try:\n                # Stream allows seeing intermediate steps\n                for step_output_update in graph.stream(current_state):\n                    node_name = list(step_output_update.keys())[0]\n                    print(f\"\\n<< Node Update: {node_name} >>\")\n                    print_agent_step(step_output_update[node_name])\n                    # Append messages to maintain history\n                    messages.extend(step_output_update[node_name].get(\"messages\",[]))\n\n                last_message = messages[-1]\n                if not (isinstance(last_message, AIMessage) and hasattr(last_message, 'tool_calls') and last_message.tool_calls):\n                    print(\"\\n--- Turn Complete (AI Response Received) ---\")\n                else:\n                    print(\"\\n--- Turn Complete (Ended on Tool Call?) ---\") # Should loop back\n\n            except Exception as e:\n                print(f\"\\nAn error occurred during agent execution: {e}\")\n                import traceback # Optional: Print full traceback for debugging\n                traceback.print_exc()\n                break # Exit loop on error\n\n            # Get next user input\n            user_input = input(\"You: \")\n            if user_input.lower() == 'quit':\n                break\n            # Append user message for the next loop turn\n            messages.append(HumanMessage(content=user_input))\n\n        print(\"\\n--- Interaction Ended ---\")\n\n\n    # --- Example Usage Section (Refactored Call) ---\n    if agent_graph:\n        print(\"\\n--- Agent Ready ---\")\n        initial_human_message = \"\"\"Hi, I need help with sustainable procurement for a project. We are procuring 1000 standard office laptops (like Dell Latitude) for a new administrative center in Nigeria (ISO: NG). The main partner is the Ministry of Education. What should I consider?\"\"\"\n        # To run the agent, uncomment the line below:\n        # ---> !!! UNCOMMENT THIS LINE TO START CHATTING !!! <---\n        # run_agent_interaction(agent_graph, initial_human_message)\n        print(\"\\n--- Interaction Loop Example Finished (Call is Commented Out) ---\")\n        print(\"To chat with the agent, uncomment the 'run_agent_interaction(...)' line above and run this cell.\")\n    else:\n        print(\"\\nAgent graph not available. Cannot start interaction loop.\")\n\n\n# --- Final Status ---\n# (Final status check remains the same)\nprint(\"\\n--- Step 14 Final Status ---\")\nif 'agent_graph' in locals() and agent_graph is not None:\n    print(\"Step 14 Completed: LangGraph agent framework 'agent_graph' is compiled.\")\n    print(\"Function 'run_agent_interaction' is defined to start chatting.\")\nelse:\n    print(\"Step 14 Incomplete: Failed to compile agent graph or prerequisites missing.\")\n\n# ==============================================================================\n# (End of Step 14 Code Block)\n# ==============================================================================","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T09:43:35.196425Z","iopub.execute_input":"2025-04-27T09:43:35.196668Z","iopub.status.idle":"2025-04-27T09:43:35.329812Z","shell.execute_reply.started":"2025-04-27T09:43:35.196642Z","shell.execute_reply":"2025-04-27T09:43:35.328869Z"}},"outputs":[{"name":"stdout","text":"Imported ToolExecutor and ToolInvocation from langgraph.prebuilt\n\n--- Loading Prerequisites for Agent Framework ---\nAttempting to define/redefine embedding function 'gemini_ef'...\nDefined Gemini embedding function.\nCollection object 'sp_criteria_collection' already exists in global scope.\nCollection object 'segment_risk_collection' already exists in global scope.\nCollection object 'country_risk_collection' already exists in global scope.\n\nPrerequisites seem OK. Defining agent framework...\nInitializing agent LLM with model: gemini-2.0-flash\n\n--- Building LangGraph Agent ---\nAgent graph compiled.\n\n--- Agent Ready ---\n\n--- Interaction Loop Example Finished (Call is Commented Out) ---\nTo chat with the agent, uncomment the 'run_agent_interaction(...)' line above and run this cell.\n\n--- Step 14 Final Status ---\nStep 14 Completed: LangGraph agent framework 'agent_graph' is compiled.\nFunction 'run_agent_interaction' is defined to start chatting.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}